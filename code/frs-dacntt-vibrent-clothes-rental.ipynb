{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"},{"sourceId":3186469,"sourceType":"datasetVersion","datasetId":1935128},{"sourceId":9334353,"sourceType":"datasetVersion","datasetId":5546448,"isSourceIdPinned":false}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### import các thư viện cần thiết","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport time\nimport itertools\n\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nprint(\"PyTorch:\", torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:13:16.135362Z","iopub.execute_input":"2025-12-04T11:13:16.136146Z","iopub.status.idle":"2025-12-04T11:13:16.141792Z","shell.execute_reply.started":"2025-12-04T11:13:16.136118Z","shell.execute_reply":"2025-12-04T11:13:16.140969Z"}},"outputs":[{"name":"stdout","text":"PyTorch: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"# Chuẩn bị dữ liệu H&M","metadata":{}},{"cell_type":"code","source":"DATASET_NAME = \"hm\"  # để log / tên file…\n\nBASE_PATH = \"/kaggle/input/h-and-m-personalized-fashion-recommendations\"\n\n# Thư mục ảnh: chỉnh thành path mà bạn đang có\n# Ví dụ nếu bạn có bộ ảnh kiểu Kaggle community:\n#   images/010/0108775015.jpg\nIMAGE_DIR = \"/kaggle/input/hm-images/images\"   # <-- sửa cho đúng của bạn\n\nCFG = {\n    \"outfits_file\": \"articles.csv\",\n    \"pictures_file\": None,   # H&M không có file ảnh riêng như Vibrent\n    \"user_activity_file\": \"transactions_train.csv\",\n    \"csv_sep\": \",\",\n\n    # cột user / item / thời gian trong transactions_train\n    \"user_col\": \"customer_id\",\n    \"item_col\": \"article_id\",\n    \"start_time_col\": \"t_dat\",\n    \"end_time_col\": \"t_dat\",\n\n    # cột “ảnh” – mình sẽ tự build dựa trên article_id\n    \"picture_outfit_col\": \"article_id\",\n    \"picture_filename_col\": \"file_name\",   # sẽ tạo giả trong code\n    \"picture_displayorder_col\": None,\n\n    # cột text để build câu mô tả cho FashionCLIP\n    \"outfit_text_cols\": [\n        \"prod_name\",\n        \"product_type_name\",\n        \"product_group_name\",\n        \"graphical_appearance_name\",\n        \"colour_group_name\",\n        \"perceived_colour_value_name\",\n        \"perceived_colour_master_name\",\n        \"department_name\",\n        \"index_name\",\n        \"index_group_name\",\n        \"section_name\",\n        \"garment_group_name\",\n        \"detail_desc\",\n    ],\n}\n\nprint(\"BASE_PATH:\", BASE_PATH)\nprint(\"IMAGE_DIR:\", IMAGE_DIR)\nprint(\"CFG:\", CFG)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:04:46.995714Z","iopub.execute_input":"2025-12-04T09:04:46.996033Z","iopub.status.idle":"2025-12-04T09:04:47.002337Z","shell.execute_reply.started":"2025-12-04T09:04:46.996009Z","shell.execute_reply":"2025-12-04T09:04:47.001587Z"}},"outputs":[{"name":"stdout","text":"BASE_PATH: /kaggle/input/h-and-m-personalized-fashion-recommendations\nIMAGE_DIR: /kaggle/input/hm-images/images\nCFG: {'outfits_file': 'articles.csv', 'pictures_file': None, 'user_activity_file': 'transactions_train.csv', 'csv_sep': ',', 'user_col': 'customer_id', 'item_col': 'article_id', 'start_time_col': 't_dat', 'end_time_col': 't_dat', 'picture_outfit_col': 'article_id', 'picture_filename_col': 'file_name', 'picture_displayorder_col': None, 'outfit_text_cols': ['prod_name', 'product_type_name', 'product_group_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'detail_desc']}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def load_dataset(base_path, cfg):\n    sep = cfg.get(\"csv_sep\", \",\")\n\n    outfits = pd.read_csv(\n        os.path.join(base_path, cfg[\"outfits_file\"]),\n        sep=sep,\n        quotechar='\"',\n        engine=\"python\",\n        on_bad_lines=\"skip\"\n    )\n\n    pictures_file = cfg.get(\"pictures_file\", None)\n    if pictures_file is not None:\n        pictures = pd.read_csv(\n            os.path.join(base_path, pictures_file),\n            sep=sep,\n            quotechar='\"',\n            engine=\"python\"\n        )\n    else:\n        pictures = None\n\n    user_activity = pd.read_csv(\n        os.path.join(base_path, cfg[\"user_activity_file\"]),\n        sep=sep,\n        quotechar='\"',\n        engine=\"python\"\n    )\n\n    print(\"Outfits (articles):\", outfits.shape)\n    print(\"Pictures:\", None if pictures is None else pictures.shape)\n    print(\"User activity (transactions):\", user_activity.shape)\n    return outfits, pictures, user_activity\n\n\noutfits, pictures, user_activity = load_dataset(BASE_PATH, CFG)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:05:04.607045Z","iopub.execute_input":"2025-12-04T09:05:04.607848Z","iopub.status.idle":"2025-12-04T09:08:28.353953Z","shell.execute_reply.started":"2025-12-04T09:05:04.607822Z","shell.execute_reply":"2025-12-04T09:08:28.353306Z"}},"outputs":[{"name":"stdout","text":"Outfits (articles): (105542, 25)\nPictures: None\nUser activity (transactions): (31788324, 5)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def article_id_to_image_path(article_id, image_root):\n    aid_str = str(article_id).zfill(10)   # H&M article_id 10 digits\n    folder = aid_str[:3]\n    filename = aid_str + \".jpg\"\n    return os.path.join(image_root, folder, filename)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:08:28.355172Z","iopub.execute_input":"2025-12-04T09:08:28.355450Z","iopub.status.idle":"2025-12-04T09:08:28.359850Z","shell.execute_reply.started":"2025-12-04T09:08:28.355433Z","shell.execute_reply":"2025-12-04T09:08:28.359114Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def load_image(path):\n    if os.path.exists(path):\n        try:\n            return Image.open(path).convert(\"RGB\")\n        except:\n            return None\n    return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:15:04.201706Z","iopub.execute_input":"2025-12-04T09:15:04.201989Z","iopub.status.idle":"2025-12-04T09:15:04.206297Z","shell.execute_reply.started":"2025-12-04T09:15:04.201967Z","shell.execute_reply":"2025-12-04T09:15:04.205662Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def build_group_images(outfits: pd.DataFrame, cfg, image_dir: str):\n    \"\"\"\n    Tạo một DataFrame mapping:\n        group_id (product_code) -> list ảnh + list article_id trong group\n    \"\"\"\n    if \"product_code\" not in outfits.columns:\n        raise ValueError(\"articles.csv không có cột product_code\")\n\n    records = []\n    for _, row in outfits.iterrows():\n        article_id = row[cfg[\"item_col\"]]       # article_id\n        group_id = row[\"product_code\"]          # nhóm trang phục\n        img_path = article_id_to_image_path(article_id, image_dir)\n        records.append({\n            \"product_code\": group_id,\n            \"article_id\": article_id,\n            \"image_path\": img_path,\n        })\n\n    df = pd.DataFrame(records)\n    return df\n\n\ngroup_images = build_group_images(outfits, CFG, IMAGE_DIR)\nprint(\"Số article có path ảnh (theo rule):\", len(group_images))\nprint(group_images.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:15:05.122440Z","iopub.execute_input":"2025-12-04T09:15:05.122706Z","iopub.status.idle":"2025-12-04T09:15:09.545617Z","shell.execute_reply.started":"2025-12-04T09:15:05.122688Z","shell.execute_reply":"2025-12-04T09:15:09.544882Z"}},"outputs":[{"name":"stdout","text":"Số article có path ảnh (theo rule): 105542\n   product_code  article_id                                         image_path\n0        108775   108775015  /kaggle/input/hm-images/images/010/0108775015.jpg\n1        108775   108775044  /kaggle/input/hm-images/images/010/0108775044.jpg\n2        108775   108775051  /kaggle/input/hm-images/images/010/0108775051.jpg\n3        110065   110065001  /kaggle/input/hm-images/images/011/0110065001.jpg\n4        110065   110065002  /kaggle/input/hm-images/images/011/0110065002.jpg\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def build_text_column(outfits: pd.DataFrame, cfg):\n    text_cols = cfg.get(\"outfit_text_cols\", [])\n\n    def _build(row):\n        parts = []\n        for c in text_cols:\n            if c in row and pd.notna(row[c]):\n                parts.append(str(row[c]))\n        return \" \".join(parts)\n\n    return outfits.apply(_build, axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:27:12.621882Z","iopub.execute_input":"2025-12-04T09:27:12.622502Z","iopub.status.idle":"2025-12-04T09:27:12.626947Z","shell.execute_reply.started":"2025-12-04T09:27:12.622479Z","shell.execute_reply":"2025-12-04T09:27:12.626284Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import sys\nimport subprocess\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"fashion-clip\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:39:20.252572Z","iopub.execute_input":"2025-12-04T09:39:20.253052Z","iopub.status.idle":"2025-12-04T09:40:42.776116Z","shell.execute_reply.started":"2025-12-04T09:39:20.253030Z","shell.execute_reply":"2025-12-04T09:40:42.775384Z"}},"outputs":[{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 5.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 106.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 83.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 37.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 1.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 5.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 35.7 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 15.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 7.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 13.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.7/44.7 kB 2.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.6/193.6 kB 11.1 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['/usr/bin/python3', '-m', 'pip', 'install', '-q', 'fashion-clip'], returncode=0)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from fashion_clip.fashion_clip import FashionCLIP\nfclip = FashionCLIP('fashion-clip')\n\nEMB_FILE_NAME = \"outfit_embeddings.npy\"\nOUTFIT_EMB_PATH = os.path.join(\"/kaggle/working\", EMB_FILE_NAME)\n\noutfit_embeddings = None\n\nif os.path.exists(OUTFIT_EMB_PATH):\n    print(\"=> Đang load outfit_embeddings từ\", OUTFIT_EMB_PATH)\n    arr = np.load(OUTFIT_EMB_PATH, allow_pickle=True)\n    if isinstance(arr, np.ndarray) and arr.dtype == object and arr.size == 1:\n        outfit_embeddings = arr.item()\n    else:\n        raise ValueError(\"Định dạng file embeddings cũ không đúng (mong đợi dict).\")\nelse:\n    print(\"=> Không tìm thấy\", OUTFIT_EMB_PATH, \"→ tính lại bằng FashionCLIP...\")\n\n    # ---- TEXT EMBEDDING ----\n    outfits[\"__text_for_clip\"] = build_text_column(outfits, CFG)\n    texts = outfits[\"__text_for_clip\"].fillna(\"\").tolist()\n    article_ids = outfits[CFG[\"item_col\"]].values  # article_id\n\n    text_embeddings = fclip.encode_text(texts, batch_size=32)\n    text_emb_map = dict(zip(article_ids, text_embeddings))\n\n    # ---- IMAGE EMBEDDING THEO NHÓM product_code ----\n    image_emb_group = {}   # product_code -> vector\n    for g, df_g in group_images.groupby(\"product_code\"):\n        imgs = []\n        for _, row in df_g.iterrows():\n            img = load_image(row[\"image_path\"])\n            if img is not None:\n                imgs.append(img)\n        if not imgs:\n            continue\n        embs = fclip.encode_images(imgs, batch_size=16)\n        image_emb_group[g] = embs.mean(axis=0)\n\n    # ---- GỘP VÀO outfit_embeddings (article level) ----\n    outfit_embeddings = {}\n    for _, row in outfits.iterrows():\n        aid = row[CFG[\"item_col\"]]            # article_id\n        g = row[\"product_code\"]               # nhóm\n        t = text_emb_map.get(aid)\n        v = image_emb_group.get(g)            # embedding nhóm\n\n        if (t is not None) and (v is not None):\n            outfit_embeddings[aid] = 0.5 * t + 0.5 * v\n        elif t is not None:\n            outfit_embeddings[aid] = t\n        elif v is not None:\n            outfit_embeddings[aid] = v\n\n    print(\"=> Lưu\", OUTFIT_EMB_PATH)\n    np.save(OUTFIT_EMB_PATH, outfit_embeddings)\n\nprint(\"Số article có embedding:\", len(outfit_embeddings))\nfeat_dim = next(iter(outfit_embeddings.values())).shape[0]\nprint(\"Kích thước embedding:\", feat_dim)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chuẩn bị dữ liệu vibrent-clothes-rental-dataset","metadata":{}},{"cell_type":"code","source":"# ========= CONFIG DATASET =========\n# Chỉ cần sửa block này khi đổi dataset\n\nDATASET_NAME = \"vibrent\"  # tên để đặt file output embeddings, log,...\n\n# local:\n# BASE_PATH = r\"D:\\DACNTT\"\n# IMAGE_DIR = os.path.join(BASE_PATH, \"images\")\n\n# Kaggle:\nBASE_PATH = \"/kaggle/input/vibrent-clothes-rental-dataset\"\nIMAGE_DIR = os.path.join(BASE_PATH, \"images\")\n# File & cột tương ứng với dataset Vibrent\nCFG = {\n    \"outfits_file\": \"outfits.csv\",\n    \"pictures_file\": \"picture_triplets.csv\",\n    \"user_activity_file\": \"user_activity_triplets.csv\",\n    \"csv_sep\": \";\",\n    # cột user / item / thời gian trong user_activity\n    \"user_col\": \"customer.id\",\n    \"item_col\": \"outfit.id\",\n    \"start_time_col\": \"rentalPeriod.start\",\n    \"end_time_col\": \"rentalPeriod.end\",\n    # cột trong pictures\n    \"picture_outfit_col\": \"outfit.id\",\n    \"picture_filename_col\": \"file_name\",\n    \"picture_displayorder_col\": \"displayOrder\",\n    # cột để xây text cho FashionCLIP\n    # sẽ join các cột này bằng khoảng trắng (bỏ NA)\n    \"outfit_text_cols\": [\"description\", \"outfit_tags\"],\n}\n\nprint(\"BASE_PATH:\", BASE_PATH)\nprint(\"IMAGE_DIR:\", IMAGE_DIR)\nprint(\"CFG:\", CFG)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:13:21.370001Z","iopub.execute_input":"2025-12-04T11:13:21.370310Z","iopub.status.idle":"2025-12-04T11:13:21.376526Z","shell.execute_reply.started":"2025-12-04T11:13:21.370289Z","shell.execute_reply":"2025-12-04T11:13:21.375524Z"}},"outputs":[{"name":"stdout","text":"BASE_PATH: /kaggle/input/vibrent-clothes-rental-dataset\nIMAGE_DIR: /kaggle/input/vibrent-clothes-rental-dataset/images\nCFG: {'outfits_file': 'outfits.csv', 'pictures_file': 'picture_triplets.csv', 'user_activity_file': 'user_activity_triplets.csv', 'csv_sep': ';', 'user_col': 'customer.id', 'item_col': 'outfit.id', 'start_time_col': 'rentalPeriod.start', 'end_time_col': 'rentalPeriod.end', 'picture_outfit_col': 'outfit.id', 'picture_filename_col': 'file_name', 'picture_displayorder_col': 'displayOrder', 'outfit_text_cols': ['description', 'outfit_tags']}\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"load dữ liệu","metadata":{}},{"cell_type":"code","source":"def load_dataset(base_path, cfg):\n    sep = cfg.get(\"csv_sep\", \",\")\n\n    outfits = pd.read_csv(\n        os.path.join(base_path, cfg[\"outfits_file\"]),\n        sep=sep,\n        quotechar='\"',\n        engine=\"python\",\n        on_bad_lines=\"skip\"\n    )\n\n    pictures = pd.read_csv(\n        os.path.join(base_path, cfg[\"pictures_file\"]),\n        sep=sep,\n        quotechar='\"',\n        engine=\"python\"\n    )\n\n    user_activity = pd.read_csv(\n        os.path.join(base_path, cfg[\"user_activity_file\"]),\n        sep=sep,\n        quotechar='\"',\n        engine=\"python\"\n    )\n\n    print(\"Outfits:\", outfits.shape)\n    print(\"Pictures:\", pictures.shape)\n    print(\"User activity:\", user_activity.shape)\n    return outfits, pictures, user_activity\n\n\noutfits, pictures, user_activity = load_dataset(BASE_PATH, CFG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:13:22.779991Z","iopub.execute_input":"2025-12-04T11:13:22.780296Z","iopub.status.idle":"2025-12-04T11:13:23.373736Z","shell.execute_reply.started":"2025-12-04T11:13:22.780276Z","shell.execute_reply":"2025-12-04T11:13:23.373033Z"}},"outputs":[{"name":"stdout","text":"Outfits: (15649, 11)\nPictures: (50193, 4)\nUser activity: (64419, 4)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"image_dir = \"/kaggle/input/vibrent-clothes-rental-dataset/images\"\n\ndef load_image(file_name):\n    path = os.path.join(image_dir, file_name)\n    if os.path.exists(path):\n        try:\n            return Image.open(path)\n        except:\n            return None\n    return None\n\n\ndef build_main_pictures(pictures: pd.DataFrame, cfg, image_dir: str):\n    outfit_col = cfg[\"picture_outfit_col\"]\n    fname_col = cfg[\"picture_filename_col\"]\n    disp_col = cfg.get(\"picture_displayorder_col\", None)\n\n    df = pictures.copy()\n\n    if disp_col is not None and disp_col in df.columns:\n        df = (\n            df.sort_values(disp_col)\n              .groupby(outfit_col)\n              .first()\n              .reset_index()\n        )\n    else:\n        # Nếu dataset khác không có displayOrder, lấy dòng đầu tiên cho mỗi outfit\n        df = (\n            df.groupby(outfit_col)\n              .first()\n              .reset_index()\n        )\n\n    df[\"image_path\"] = df[fname_col].apply(lambda fn: os.path.join(image_dir, fn))\n    return df\n\n\nmain_pictures = build_main_pictures(pictures, CFG, IMAGE_DIR)\nprint(\"Số outfit có main picture (theo CSV):\", len(main_pictures))\nprint(main_pictures.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:13:23.842663Z","iopub.execute_input":"2025-12-04T11:13:23.842938Z","iopub.status.idle":"2025-12-04T11:13:23.920671Z","shell.execute_reply.started":"2025-12-04T11:13:23.842918Z","shell.execute_reply":"2025-12-04T11:13:23.919900Z"}},"outputs":[{"name":"stdout","text":"Số outfit có main picture (theo CSV): 15157\n                                 outfit.id  \\\n0  outfit.00004b4d01ca4ab0a70cf073ba74fefa   \n1  outfit.0013691ff35b440e9dcfe1748ec184c7   \n2  outfit.0014a5c89b244077a3d7cffd4549718e   \n3  outfit.0018701ce6b049ebadc314d16623caa8   \n4  outfit.001bf665330140cf854dcfb1cbff6b5f   \n\n                                 picture.id  displayOrder  \\\n0  picture.a2b794c7ef83495a8997e7b0c318d65a             1   \n1  picture.9c821ecbecb14c959f35078010fb91f3             1   \n2  picture.b9aa39eb40f5410fa4fe101236241b19             1   \n3  picture.b944a50f20fd4c7f954213dc7c38a776             1   \n4  picture.fb1ff67a0bbc418b88ebb5560fac88a1             0   \n\n                              file_name  \\\n0  a2b794c7ef83495a8997e7b0c318d65a.jpg   \n1  9c821ecbecb14c959f35078010fb91f3.jpg   \n2  b9aa39eb40f5410fa4fe101236241b19.jpg   \n3  b944a50f20fd4c7f954213dc7c38a776.jpg   \n4  fb1ff67a0bbc418b88ebb5560fac88a1.jpg   \n\n                                          image_path  \n0  /kaggle/input/vibrent-clothes-rental-dataset/i...  \n1  /kaggle/input/vibrent-clothes-rental-dataset/i...  \n2  /kaggle/input/vibrent-clothes-rental-dataset/i...  \n3  /kaggle/input/vibrent-clothes-rental-dataset/i...  \n4  /kaggle/input/vibrent-clothes-rental-dataset/i...  \n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"# Tạo outfitsEmbedding hoặc load lên nếu đã có","metadata":{}},{"cell_type":"code","source":"import sys\nimport subprocess\n\n# Cài fashion-clip nếu chưa có\n#subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"fashion-clip\"])\n\n# from fashion_clip.fashion_clip import FashionCLIP\n\n# fclip = FashionCLIP('fashion-clip')\n\nEMB_FILE_NAME = f\"/kaggle/working/outfit_embeddings.npy\"\nOUTFIT_EMB_PATH = os.path.join(BASE_PATH, EMB_FILE_NAME)\n\noutfit_embeddings = None\n\ndef build_text_column(outfits: pd.DataFrame, cfg):\n    text_cols = cfg.get(\"outfit_text_cols\", [])\n    def _build(row):\n        parts = []\n        for c in text_cols:\n            if c in row and pd.notna(row[c]):\n                parts.append(str(row[c]))\n        return \" \".join(parts)\n    return outfits.apply(_build, axis=1)\n\n\nif os.path.exists(OUTFIT_EMB_PATH):\n    print(\"=> Đang load outfit_embeddings từ\", OUTFIT_EMB_PATH)\n    arr = np.load(OUTFIT_EMB_PATH, allow_pickle=True)\n    print(\"  - Loaded shape:\", getattr(arr, \"shape\", None), \"| dtype:\", getattr(arr, \"dtype\", None))\n\n    if isinstance(arr, np.ndarray) and arr.dtype == object and arr.size == 1:\n        # Trường hợp đã lưu dict bằng np.save(dict)\n        outfit_embeddings = arr.item()\n        print(\"  - Giải mã thành dict, số outfit:\", len(outfit_embeddings))\n\n    elif isinstance(arr, np.ndarray) and arr.ndim == 2:\n        # Trường hợp đã lưu thuần mảng (N, D): ta cần gắn lại với outfit_id\n        outfit_col = CFG[\"picture_outfit_col\"]\n        outfit_ids = main_pictures[outfit_col].values\n\n        if arr.shape[0] != len(outfit_ids):\n            raise ValueError(\n                f\"Số embedding ({arr.shape[0]}) khác số main_pictures ({len(outfit_ids)}). \"\n                \"Cần chắc chắn thứ tự khi encode và khi load giống nhau.\"\n            )\n\n        outfit_embeddings = {\n            oid: emb for oid, emb in zip(outfit_ids, arr)\n        }\n        print(\"  - Đã build dict outfit_embeddings từ array: số outfit =\", len(outfit_embeddings))\n\n    else:\n        raise ValueError(\n            f\"File {EMB_FILE_NAME} có kiểu không hỗ trợ (shape={getattr(arr, 'shape', None)}, \"\n            f\"dtype={getattr(arr, 'dtype', None)}).\"\n        )\n\nelse:\n    print(\"=> Không tìm thấy\", EMB_FILE_NAME, \"→ tính lại bằng FashionCLIP...\")\n\n    # Text embedding cho outfit\n    outfits[\"__text_for_clip\"] = build_text_column(outfits, CFG)\n    texts = outfits[\"__text_for_clip\"].fillna(\"\").tolist()\n    text_embeddings = fclip.encode_text(texts, batch_size=32)\n    text_emb_map = dict(zip(outfits[\"id\"], text_embeddings))\n\n    # Image embedding cho main picture\n    images = []\n    image_outfit_ids = []\n\n    outfit_col = CFG[\"picture_outfit_col\"]\n    fname_col = CFG[\"picture_filename_col\"]\n\n    for _, row in main_pictures.iterrows():\n        oid = row[outfit_col]\n        img = load_image(row[fname_col], IMAGE_DIR)\n        if img is not None:\n            images.append(img)\n            image_outfit_ids.append(oid)\n\n    print(\"Số ảnh encode được:\", len(images))\n    if images:\n        image_embeddings = fclip.encode_images(images, batch_size=32)\n        image_emb_map = dict(zip(image_outfit_ids, image_embeddings))\n    else:\n        image_emb_map = {}\n\n    # Gộp text + image -> outfit_embeddings\n    outfit_embeddings = {}\n    for oid in outfits[\"id\"]:\n        t = text_emb_map.get(oid)\n        v = image_emb_map.get(oid)\n        if t is not None and v is not None:\n            outfit_embeddings[oid] = 0.5 * t + 0.5 * v\n        elif t is not None:\n            outfit_embeddings[oid] = t\n        elif v is not None:\n            outfit_embeddings[oid] = v\n\n    print(\"=> Lưu\", EMB_FILE_NAME, \"vào\", BASE_PATH)\n    np.save(OUTFIT_EMB_PATH, outfit_embeddings)\n\nprint(\"Số outfit có embedding:\", len(outfit_embeddings))\nfeat_dim = next(iter(outfit_embeddings.values())).shape[0]\nprint(\"Kích thước embedding:\", feat_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:13:26.325590Z","iopub.execute_input":"2025-12-04T11:13:26.326162Z","iopub.status.idle":"2025-12-04T11:13:26.363082Z","shell.execute_reply.started":"2025-12-04T11:13:26.326142Z","shell.execute_reply":"2025-12-04T11:13:26.362372Z"}},"outputs":[{"name":"stdout","text":"=> Đang load outfit_embeddings từ /kaggle/working/outfit_embeddings.npy\n  - Loaded shape: (15157, 512) | dtype: float32\n  - Đã build dict outfit_embeddings từ array: số outfit = 15157\nSố outfit có embedding: 15157\nKích thước embedding: 512\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"Hàm lấy outfits từ text ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef find_outfits_from_text(query_text, outfit_embeddings, model, topk=10):\n    \"\"\"\n    Tìm top-K outfit phù hợp với text (query) dựa trên FashionCLIP embedding.\n    \"\"\"\n    # 1) Encode text thành embedding\n    text_emb = model.encode_text([query_text], batch_size=1)[0]\n    text_emb = text_emb.astype(np.float32)\n\n    # L2 normalize để cosine similarity\n    text_emb = text_emb / (np.linalg.norm(text_emb) + 1e-8)\n\n    # 2) Chuẩn bị item embedding matrix\n    outfit_ids = list(outfit_embeddings.keys())\n    item_matrix = np.vstack([outfit_embeddings[oid] for oid in outfit_ids]).astype(np.float32)\n\n    # Normalize item embeddings\n    item_matrix = item_matrix / (np.linalg.norm(item_matrix, axis=1, keepdims=True) + 1e-8)\n\n    # 3) Tính cosine similarity\n    scores = item_matrix @ text_emb     # shape = [num_items]\n\n    # 4) Lấy top-k\n    top_idx = np.argsort(-scores)[:topk]\n\n    results = [(outfit_ids[i], float(scores[i])) for i in top_idx]\n    return results\ndef get_images_for_query(query_text, topk=5):\n    results = find_outfits_from_text(query_text, outfit_embeddings, fclip, topk=topk)\n\n    pics = []\n    for oid, score in results:\n        row = main_pictures[main_pictures[\"outfit.id\"] == oid]\n        if len(row) > 0:\n            file = row.iloc[0][\"file_name\"]\n            img = load_image(file)\n            pics.append((img, oid, score))\n    return pics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:13:29.082776Z","iopub.execute_input":"2025-12-04T11:13:29.083352Z","iopub.status.idle":"2025-12-04T11:13:29.090634Z","shell.execute_reply.started":"2025-12-04T11:13:29.083324Z","shell.execute_reply":"2025-12-04T11:13:29.089893Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from IPython.display import display\n\nquery = \"\"\npics = get_images_for_query(query, topk=5)\n\nfor img, oid, score in pics:\n    print(f\"Outfit: {oid} | score={score:.3f}\")\n    display(img)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:49:59.383258Z","iopub.status.idle":"2025-12-04T09:49:59.383469Z","shell.execute_reply.started":"2025-12-04T09:49:59.383369Z","shell.execute_reply":"2025-12-04T09:49:59.383378Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"{\"timestamp\":\"2025-12-04T09:50:02.938906Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n{\"timestamp\":\"2025-12-04T09:50:02.938940Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #4. Sleeping 24.820109788s before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n{\"timestamp\":\"2025-12-04T09:50:04.465757Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n{\"timestamp\":\"2025-12-04T09:50:04.465779Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #4. Sleeping 23.296358132s before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n{\"timestamp\":\"2025-12-04T09:50:07.216044Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n{\"timestamp\":\"2025-12-04T09:50:07.216073Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #4. Sleeping 599.202931ms before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n{\"timestamp\":\"2025-12-04T09:50:07.741232Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n{\"timestamp\":\"2025-12-04T09:50:07.741255Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #4. Sleeping 22.562472467s before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n{\"timestamp\":\"2025-12-04T09:50:07.920734Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n{\"timestamp\":\"2025-12-04T09:50:07.920767Z\",\"level\":\"ERROR\",\"fields\":{\"message\":\"error code, error: ReqwestError(reqwest::Error { kind: Status(429, None) }, \\\"https://us.gcp.cdn.hf.co/xorbs/default/8e673d3d99b9f973d426df47f01c12061475730e361ec35bee995734c74eeb68\\\")\",\"caller\":\"/home/runner/work/xet-core/xet-core/cas_client/src/download_utils.rs:539\"},\"filename\":\"/home/runner/work/xet-core/xet-core/error_printer/src/lib.rs\",\"line_number\":28}\n{\"timestamp\":\"2025-12-04T09:50:09.476971Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n{\"timestamp\":\"2025-12-04T09:50:09.476994Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #4. Sleeping 23.927883948s before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n{\"timestamp\":\"2025-12-04T09:50:15.429559Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n{\"timestamp\":\"2025-12-04T09:50:15.429593Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #4. Sleeping 39.436089187s before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# BERT+ImageEmbedding có sẵn","metadata":{}},{"cell_type":"code","source":"# import sys\n# import subprocess\n# subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\"])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-24T07:02:31.792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from transformers import BertTokenizer, BertModel\n# import torch\n# from tqdm import tqdm\n\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n# bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n# bert_model.eval()\n\n# def encode_texts_bert(texts, batch_size=16):\n#     embeddings = []\n#     for i in range(0, len(texts), batch_size):\n#         batch = texts[i:i+batch_size]\n#         tokens = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=64).to(device)\n#         with torch.no_grad():\n#             outputs = bert_model(**tokens)\n#             cls_emb = outputs.pooler_output  # vector [CLS]\n#             cls_emb = torch.nn.functional.normalize(cls_emb, p=2, dim=1)\n#             embeddings.append(cls_emb.cpu().numpy())\n#     return np.vstack(embeddings)\n\n# # === Tạo embedding văn bản bằng BERT ===\n# texts = outfits[\"text\"].fillna(\"\").tolist()\n# bert_text_embeddings = encode_texts_bert(texts, batch_size=16)\n# print(\"BERT embedding shape:\", bert_text_embeddings.shape)\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-24T07:02:31.792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# embedding_dir = os.path.join(base_path, \"embeddings\", \"EfficientNet_V2_L_final\")\n# dataset_image_embeddings = {}\n# for _, row in main_pictures.iterrows():\n#     picture_id = row[\"picture.id\"]\n#     file_path = os.path.join(embedding_dir, f\"picture.{picture_id}.npy\")\n#     if os.path.exists(file_path):\n#         try:\n#             vec = np.load(file_path)\n#             dataset_image_embeddings[row[\"outfit.id\"]] = vec\n#         except Exception as e:\n#             print(f\"Lỗi load {file_path}: {e}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-24T07:02:31.792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CFRS ","metadata":{}},{"cell_type":"markdown","source":"Sort giao dịch user theo thời gian và mapping","metadata":{}},{"cell_type":"code","source":"#CF preprocessing – encode user/item, split train/test, build A_train\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nu_col = CFG[\"user_col\"]\ni_col = CFG[\"item_col\"]\nt_start = CFG[\"start_time_col\"]\nt_end = CFG[\"end_time_col\"]\n\n# 1) Parse thời gian & sort\nfor col in [t_start, t_end]:\n    user_activity[col] = pd.to_datetime(user_activity[col], errors=\"coerce\", utc=True)\nuser_activity = user_activity.dropna(subset=[t_start, t_end])\nuser_activity = user_activity.sort_values([u_col, t_start, t_end])\nprint(\"[OK] Parse & sort thời gian.\")\n\n# 2) Encode user / item\nuser_enc = LabelEncoder().fit(user_activity[u_col].astype(str))\nitem_enc = LabelEncoder().fit(user_activity[i_col].astype(str))\n\nuser_activity[\"user_id\"] = user_enc.transform(user_activity[u_col].astype(str))\nuser_activity[\"item_id\"] = item_enc.transform(user_activity[i_col].astype(str))\n\nnum_users = user_activity[\"user_id\"].nunique()\nnum_items = user_activity[\"item_id\"].nunique()\n\n# mapping giữa item_id (index trong matrix) và outfit-id gốc\noid_from_item = dict(zip(user_activity[\"item_id\"], user_activity[i_col]))\nitem_from_oid = {v: k for k, v in oid_from_item.items()}\n\nprint(f\"[COUNTS] users={num_users} | items={num_items} | interactions={len(user_activity)}\")\n\n# 3) Chia train/test theo thời gian cho từng user\nTEST_RATIO = 0.2\nuser_train_items, user_test_items = {}, {}\n\nfor u, g in user_activity.groupby(\"user_id\", sort=False):\n    g = g.sort_values([t_start, t_end])\n    items = list(g[\"item_id\"])\n    if len(items) < 3:\n        continue\n    test_size = max(1, int(len(items) * TEST_RATIO))\n    user_train_items[u] = items[:-test_size]\n    user_test_items[u] = items[-test_size:]\n\nprint(f\"[SPLIT] users trong split = {len(user_train_items)}\")\n\n# Loại user không còn test hợp lệ\ntrain_item_set = set(i for items in user_train_items.values() for i in items)\nfor u in list(user_test_items.keys()):\n    filtered = [i for i in user_test_items[u] if i in train_item_set]\n    if len(filtered) == 0:\n        user_test_items.pop(u, None)\n        user_train_items.pop(u, None)\n    else:\n        user_test_items[u] = filtered\n\nprint(f\"[SPLIT after filter] users = {len(user_train_items)}\")\n\n# 4) Build A_train (CSR)\nrows, cols = [], []\nfor u, items in user_train_items.items():\n    rows.extend([u] * len(items))\n    cols.extend(items)\n\nif not rows:\n    raise RuntimeError(\"Không còn tương tác train nào sau khi split.\")\n\nA_train = csr_matrix(\n    (np.ones(len(rows), dtype=np.float32),\n     (np.array(rows), np.array(cols))),\n    shape=(num_users, num_items),\n    dtype=np.float32\n)\nA_train.data[:] = 1.0\nA_train.eliminate_zeros()\n\nprint(f\"[MATRIX] A_train shape={A_train.shape} | nnz={A_train.nnz}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:34:16.814972Z","iopub.execute_input":"2025-12-04T11:34:16.815342Z","iopub.status.idle":"2025-12-04T11:34:18.913204Z","shell.execute_reply.started":"2025-12-04T11:34:16.815319Z","shell.execute_reply":"2025-12-04T11:34:18.912436Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n[OK] Parse & sort thời gian.\n[COUNTS] users=2293 | items=10986 | interactions=64419\n[SPLIT] users trong split = 1979\n[SPLIT after filter] users = 1956\n[MATRIX] A_train shape=(2293, 10986) | nnz=47356\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"#Các hàm cần thiết\nclass BPRLoss(nn.Module):\n    def __init__(self, reg_lambda=1e-4):\n        super().__init__()\n        self.reg_lambda = reg_lambda\n\n    def forward(self, user_emb, pos_emb, neg_emb):\n        pos_score = torch.sum(user_emb * pos_emb, dim=1)\n        neg_score = torch.sum(user_emb * neg_emb, dim=1)\n        diff = pos_score - neg_score\n        bpr = F.softplus(-diff).mean()\n        reg = (\n            user_emb.norm(2).pow(2)\n            + pos_emb.norm(2).pow(2)\n            + neg_emb.norm(2).pow(2)\n        ) / user_emb.size(0)\n        return bpr + self.reg_lambda * reg\n\n\ndef l2n_t(x, eps=1e-8):\n    return x / (x.norm(dim=1, keepdim=True) + eps)\n\n\ndef sample_triplets(user_train_items, num_items, n_samples_per_user=5):\n    users, pos_items, neg_items = [], [], []\n    for u, pos_list in user_train_items.items():\n        if not pos_list:\n            continue\n        pos_set = set(pos_list)\n        for _ in range(n_samples_per_user):\n            p = random.choice(pos_list)\n            n = np.random.randint(0, num_items)\n            while n in pos_set:\n                n = np.random.randint(0, num_items)\n            users.append(u)\n            pos_items.append(p)\n            neg_items.append(n)\n    if not users:\n        users, pos_items, neg_items = [0], [0], [1]\n    return (\n        torch.tensor(users, dtype=torch.long),\n        torch.tensor(pos_items, dtype=torch.long),\n        torch.tensor(neg_items, dtype=torch.long),\n    )\n\n\n@torch.no_grad()\ndef build_item_neighbors(item_emb, topk=100, batch=2048):\n    item_emb = l2n_t(item_emb)\n    N, _ = item_emb.shape\n    all_topk = []\n    for i0 in range(0, N, batch):\n        xb = item_emb[i0 : i0 + batch]\n        scores = xb @ item_emb.T\n        for b in range(scores.shape[0]):\n            idx = i0 + b\n            if idx < N:\n                scores[b, idx] = -1e9\n        _, idxs = torch.topk(scores, k=min(topk, N - 1), dim=1)\n        all_topk.append(idxs.cpu().numpy())\n    return np.vstack(all_topk)\n\n\ndef hard_negative_sampling_from_neighbors(user_train_items, neighbors_idx, n_samples_per_user=5):\n    users, pos_items, neg_items = [], [], []\n    for u, pos_list in user_train_items.items():\n        if not pos_list:\n            continue\n        pos_set = set(pos_list)\n        for _ in range(n_samples_per_user):\n            p = random.choice(pos_list)\n            cand = neighbors_idx[p]\n            cand = [c for c in cand if c not in pos_set]\n            if not cand:\n                continue\n            n = random.choice(cand)\n            users.append(u)\n            pos_items.append(p)\n            neg_items.append(n)\n    if not users:\n        return sample_triplets(user_train_items, num_items, n_samples_per_user)\n    return (\n        torch.tensor(users, dtype=torch.long),\n        torch.tensor(pos_items, dtype=torch.long),\n        torch.tensor(neg_items, dtype=torch.long),\n    )\n\n\ndef mixed_negative_sampling(user_train_items, num_items, item_emb=None, neighbors_idx=None,\n                            n_samples_per_user=10, mix=0.5):\n    if (item_emb is not None) and (neighbors_idx is not None) and (np.random.rand() < mix):\n        return hard_negative_sampling_from_neighbors(user_train_items, neighbors_idx, n_samples_per_user)\n    else:\n        return sample_triplets(user_train_items, num_items, n_samples_per_user)\n\n\ndef build_norm_adj_sparse(A_csr: csr_matrix) -> sp.csr_matrix:\n    n_users, n_items = A_csr.shape\n    R = A_csr.tocsr()\n    upper = sp.hstack([sp.csr_matrix((n_users, n_users), dtype=np.float32), R], format=\"csr\")\n    lower = sp.hstack([R.T, sp.csr_matrix((n_items, n_items), dtype=np.float32)], format=\"csr\")\n    adj = sp.vstack([upper, lower], format=\"csr\").astype(np.float32)\n\n    deg = np.array(adj.sum(axis=1)).flatten().astype(np.float32)\n    deg_inv_sqrt = np.zeros_like(deg, dtype=np.float32)\n    mask = deg > 0\n    deg_inv_sqrt[mask] = np.power(deg[mask], -0.5, dtype=np.float32)\n    D_inv_sqrt = sp.diags(deg_inv_sqrt, format=\"csr\", dtype=np.float32)\n\n    norm_adj = D_inv_sqrt @ adj @ D_inv_sqrt\n    return norm_adj.tocsr()\n\n\ndef scipy_to_torch_sparse(mat: sp.csr_matrix) -> torch.Tensor:\n    coo = mat.tocoo()\n    idx = np.vstack([coo.row, coo.col]).astype(np.int64)\n    indices = torch.from_numpy(idx)\n    values = torch.from_numpy(coo.data.astype(np.float32))\n    return torch.sparse_coo_tensor(indices, values, coo.shape).coalesce()\n\n\ndef evaluate_embeddings(user_emb, item_emb, user_train_items, user_test_items, k=10):\n    U = l2n_t(user_emb.cpu())\n    I = l2n_t(item_emb.cpu())\n    I_np = I.numpy()\n\n    precisions, recalls, ndcgs, hit_rates = [], [], [], []\n\n    for u in user_test_items.keys():\n        train_items = set(user_train_items.get(u, []))\n        test_items = set(user_test_items.get(u, []))\n        if not test_items:\n            continue\n\n        scores = cosine_similarity(U[u].unsqueeze(0).numpy(), I_np)[0]\n        if train_items:\n            scores[list(train_items)] = -1e9\n\n        top_k = np.argpartition(scores, -k)[-k:]\n        top_k = top_k[np.argsort(scores[top_k])[::-1]]\n\n        hits = len(set(top_k) & test_items)\n\n        # ---- Recall/Precision như cũ ----\n        precision = hits / k\n        recall = hits / len(test_items)\n\n        # ---- NDCG như cũ ----\n        dcg = sum(1 / np.log2(i + 2) for i, iid in enumerate(top_k) if iid in test_items)\n        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(test_items), k)))\n        ndcg = dcg / idcg if idcg > 0 else 0\n\n        # ---- HitRate@K = 1 nếu có ít nhất 1 hit ----\n        hit = 1.0 if hits > 0 else 0.0\n\n        precisions.append(precision)\n        recalls.append(recall)\n        ndcgs.append(ndcg)\n        hit_rates.append(hit)\n\n    return {\n        f\"Precision@{k}\": float(np.mean(precisions)),\n        f\"Recall@{k}\": float(np.mean(recalls)),\n        f\"NDCG@{k}\": float(np.mean(ndcgs)),\n        f\"HitRate@{k}\": float(np.mean(hit_rates)),\n        \"Users_eval\": int(len(precisions)),\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T09:58:32.674133Z","iopub.execute_input":"2025-12-04T09:58:32.675006Z","iopub.status.idle":"2025-12-04T09:58:32.715934Z","shell.execute_reply.started":"2025-12-04T09:58:32.674971Z","shell.execute_reply":"2025-12-04T09:58:32.715221Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Item–item similarity từ FashionCLIP + build norm_adj_dev\n\nfeat_dim = next(iter(outfit_embeddings.values())).shape[0]\nitem_feat = np.zeros((num_items, feat_dim), dtype=np.float32)\nhas_emb = np.zeros(num_items, dtype=bool)\n\nfor item_id, oid in oid_from_item.items():\n    v = outfit_embeddings.get(oid)\n    if v is not None:\n        item_feat[item_id] = v.astype(np.float32)\n        has_emb[item_id] = True\n\nprint(\"Số item có embedding CLIP:\", has_emb.sum(), \"/\", num_items)\n\nitem_feat = item_feat / (np.linalg.norm(item_feat, axis=1, keepdims=True) + 1e-8)\n\n# Dense similarity (sau đó sparse hóa top-K)\nsim_item_item = item_feat @ item_feat.T\nprint(\"Shape sim_item_item:\", sim_item_item.shape)\n\nK = 50\nrows, cols, vals = [], [], []\nfor i in range(num_items):\n    row = sim_item_item[i]\n    topk_idx = np.argpartition(-row, min(K + 1, num_items - 1))[: K + 1]\n    topk_idx = topk_idx[topk_idx != i]\n    topk_idx = topk_idx[:K]\n    for j in topk_idx:\n        sim_ij = float(row[j])\n        if sim_ij > 0:\n            rows.append(i)\n            cols.append(j)\n            vals.append(sim_ij)\n\nS_sparse = csr_matrix((vals, (rows, cols)), shape=(num_items, num_items), dtype=np.float32)\nprint(\"S_sparse: shape =\", S_sparse.shape,\n      \"| nnz =\", S_sparse.nnz,\n      \"| avg neighbors per item ~\", S_sparse.nnz / num_items)\n\n# Filter user + remap, build A_train_new và norm_adj_dev (tái dùng giữa dataset)\nMIN_TRAIN = 3\nMIN_TEST = 1\n\nkept_users = [\n    u for u in user_train_items.keys()\n    if len(user_train_items[u]) >= MIN_TRAIN and len(user_test_items.get(u, [])) >= MIN_TEST\n]\nprint(f\"[FILTER] kept users: {len(kept_users)} / {len(user_train_items)}\")\n\nold2new_user = {u_old: i for i, u_old in enumerate(sorted(kept_users))}\nnew2old_user = {i: u_old for u_old, i in old2new_user.items()}\nnum_users_new = len(old2new_user)\n\nuser_train_items_new, user_test_items_new = {}, {}\nfor u_old in kept_users:\n    u_new = old2new_user[u_old]\n    user_train_items_new[u_new] = list(user_train_items[u_old])\n    user_test_items_new[u_new] = list(user_test_items[u_old])\n\nrows, cols = [], []\nfor u_new, items in user_train_items_new.items():\n    rows.extend([u_new] * len(items))\n    cols.extend(items)\n\nA_train_new = csr_matrix(\n    (np.ones(len(rows), dtype=np.float32), (np.array(rows), np.array(cols))),\n    shape=(num_users_new, num_items),\n    dtype=np.float32\n)\nA_train_new.data[:] = 1.0\nA_train_new.eliminate_zeros()\nprint(f\"[MATRIX] A_train_new shape={A_train_new.shape} | nnz={A_train_new.nnz}\")\n\nlambda_s = 0.3\nA_train_soft = A_train_new @ S_sparse\nA_train_soft = A_train_new + lambda_s * A_train_soft\nA_train_soft.eliminate_zeros()\n\nA_for_adj = A_train_new      \n# A_for_adj = A_train_soft\n\nnorm_adj_sp_new = build_norm_adj_sparse(A_for_adj)\nnorm_adj_new = scipy_to_torch_sparse(norm_adj_sp_new)\n\nuser_train_items = user_train_items_new\nuser_test_items = user_test_items_new\nA_train = A_train_new\nnum_users = num_users_new\n\nnorm_adj_dev = norm_adj_new.to(device)\nprint(\"[READY] norm_adj_dev shape:\", norm_adj_dev.shape,\n      \"| num_users + num_items =\", num_users + num_items)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:48:01.620562Z","iopub.execute_input":"2025-12-04T11:48:01.620876Z","iopub.status.idle":"2025-12-04T11:48:04.288310Z","shell.execute_reply.started":"2025-12-04T11:48:01.620855Z","shell.execute_reply":"2025-12-04T11:48:04.287557Z"}},"outputs":[{"name":"stdout","text":"Số item có embedding CLIP: 10980 / 10986\nShape sim_item_item: (10986, 10986)\nS_sparse: shape = (10986, 10986) | nnz = 549000 | avg neighbors per item ~ 49.97269251774986\n[FILTER] kept users: 1828 / 1956\n[MATRIX] A_train_new shape=(1828, 10986) | nnz=47102\n[READY] norm_adj_dev shape: torch.Size([12814, 12814]) | num_users + num_items = 12814\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"class NGCF(nn.Module):\n    def __init__(self, n_users, n_items, dim=128, layers=1, dropout=0.2):\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.dim = dim\n        self.layers = layers\n\n        self.embedding = nn.Embedding(n_users + n_items, dim)\n        nn.init.xavier_uniform_(self.embedding.weight)\n\n        self.W1 = nn.ModuleList([nn.Linear(dim, dim) for _ in range(layers)])\n        self.W2 = nn.ModuleList([nn.Linear(dim, dim) for _ in range(layers)])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, norm_adj):\n        x = self.embedding.weight\n        embs = [x]\n        for k in range(self.layers):\n            side = torch.sparse.mm(norm_adj, x)\n            sum_emb = self.W1[k](side + x)\n            bi_emb = self.W2[k](side * x)\n            x = F.leaky_relu(sum_emb + bi_emb, 0.2)\n            x = self.dropout(x)\n            x = F.normalize(x, dim=1)\n            embs.append(x)\n        return torch.cat(embs, dim=1)\n\n\n\nngcf = NGCF(num_users, num_items, dim=128, layers=2, dropout=0.1).to(device)\noptimizer = torch.optim.AdamW(ngcf.parameters(), lr=0.01, weight_decay=1e-4)\nbpr = BPRLoss(reg_lambda=1e-4)\n\nepochs = 200\nrefresh_every = 5\nn_samples_per_user = 20\nhard_ratio = 0.3\n\nloss_history_NGCF = []\nneighbors_idx = None\n\nprint(\"Training NGCF ...\")\nfor epoch in range(epochs):\n    ngcf.train()\n\n    with torch.no_grad():\n        emb_cache = ngcf(norm_adj_dev)\n        item_emb_cache = emb_cache[num_users:]\n        if (epoch == 0) or (epoch % refresh_every == 0):\n            neighbors_idx = build_item_neighbors(item_emb_cache, topk=100, batch=1024)\n\n    users, pos_items, neg_items = mixed_negative_sampling(\n        user_train_items,\n        num_items,\n        item_emb=item_emb_cache,\n        neighbors_idx=neighbors_idx,\n        n_samples_per_user=n_samples_per_user,\n        mix=hard_ratio,\n    )\n    users = users.to(device)\n    pos_items = pos_items.to(device)\n    neg_items = neg_items.to(device)\n\n    emb = ngcf(norm_adj_dev)\n    u_emb = emb[:num_users]\n    i_emb = emb[num_users:]\n\n    user_batch_emb = u_emb[users]\n    pos_emb = i_emb[pos_items]\n    neg_emb = i_emb[neg_items]\n\n    loss = bpr(user_batch_emb, pos_emb, neg_emb)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    loss_history_NGCF.append(loss.item())\n\n    if (epoch + 1) % 5 == 0:\n        ngcf.eval()\n        with torch.no_grad():\n            emb_eval = ngcf(norm_adj_dev)\n            u_eval = emb_eval[:num_users]\n            i_eval = emb_eval[num_users:]\n            metrics = evaluate_embeddings(\n                u_eval, i_eval, user_train_items, user_test_items, k=10\n            )\n        print(\n            f\"Epoch {epoch+1:03d} | loss={loss.item():.4f} | \"\n            f\"HR@10={metrics['HitRate@10']:.4f} | \"\n            f\"Recall@10={metrics['Recall@10']:.4f} | \"\n            f\"NDCG@10={metrics['NDCG@10']:.4f} | \"\n            f\"Precision@10={metrics['Precision@10']:.4f} | \"\n            f\"Users_eval={metrics['Users_eval']}\"\n        )\n\nngcf.eval()\nwith torch.no_grad():\n    emb_final = ngcf(norm_adj_dev)\n    user_emb_ngcf = emb_final[:num_users]\n    item_emb_ngcf = emb_final[num_users:]\n\nprint(\"Done. user_emb_ngcf:\", user_emb_ngcf.shape,\n      \"| item_emb_ngcf:\", item_emb_ngcf.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T10:01:40.666803Z","iopub.execute_input":"2025-12-04T10:01:40.667720Z","iopub.status.idle":"2025-12-04T10:18:06.413971Z","shell.execute_reply.started":"2025-12-04T10:01:40.667688Z","shell.execute_reply":"2025-12-04T10:18:06.413075Z"}},"outputs":[{"name":"stdout","text":"Training NGCF ...\nEpoch 005 | loss=0.4054 | HR@10=0.0126 | Recall@10=0.0028 | NDCG@10=0.0019 | Precision@10=0.0013 | Users_eval=1828\nEpoch 010 | loss=0.2931 | HR@10=0.0159 | Recall@10=0.0031 | NDCG@10=0.0030 | Precision@10=0.0016 | Users_eval=1828\nEpoch 015 | loss=0.4682 | HR@10=0.0181 | Recall@10=0.0040 | NDCG@10=0.0031 | Precision@10=0.0019 | Users_eval=1828\nEpoch 020 | loss=0.1993 | HR@10=0.0202 | Recall@10=0.0050 | NDCG@10=0.0037 | Precision@10=0.0020 | Users_eval=1828\nEpoch 025 | loss=0.4036 | HR@10=0.0235 | Recall@10=0.0055 | NDCG@10=0.0044 | Precision@10=0.0024 | Users_eval=1828\nEpoch 030 | loss=0.1396 | HR@10=0.0208 | Recall@10=0.0050 | NDCG@10=0.0041 | Precision@10=0.0021 | Users_eval=1828\nEpoch 035 | loss=0.3092 | HR@10=0.0263 | Recall@10=0.0048 | NDCG@10=0.0043 | Precision@10=0.0026 | Users_eval=1828\nEpoch 155 | loss=0.0107 | HR@10=0.0186 | Recall@10=0.0035 | NDCG@10=0.0031 | Precision@10=0.0020 | Users_eval=1828\nEpoch 160 | loss=0.0356 | HR@10=0.0197 | Recall@10=0.0036 | NDCG@10=0.0031 | Precision@10=0.0021 | Users_eval=1828\nEpoch 165 | loss=0.0334 | HR@10=0.0208 | Recall@10=0.0039 | NDCG@10=0.0033 | Precision@10=0.0022 | Users_eval=1828\nEpoch 170 | loss=0.0093 | HR@10=0.0230 | Recall@10=0.0038 | NDCG@10=0.0035 | Precision@10=0.0024 | Users_eval=1828\nEpoch 175 | loss=0.0302 | HR@10=0.0224 | Recall@10=0.0035 | NDCG@10=0.0037 | Precision@10=0.0024 | Users_eval=1828\nEpoch 180 | loss=0.0087 | HR@10=0.0219 | Recall@10=0.0034 | NDCG@10=0.0035 | Precision@10=0.0023 | Users_eval=1828\nEpoch 185 | loss=0.0083 | HR@10=0.0208 | Recall@10=0.0031 | NDCG@10=0.0034 | Precision@10=0.0022 | Users_eval=1828\nEpoch 190 | loss=0.0262 | HR@10=0.0208 | Recall@10=0.0031 | NDCG@10=0.0033 | Precision@10=0.0022 | Users_eval=1828\nEpoch 195 | loss=0.0254 | HR@10=0.0202 | Recall@10=0.0031 | NDCG@10=0.0033 | Precision@10=0.0021 | Users_eval=1828\nEpoch 200 | loss=0.0077 | HR@10=0.0197 | Recall@10=0.0031 | NDCG@10=0.0033 | Precision@10=0.0021 | Users_eval=1828\nDone. user_emb_ngcf: torch.Size([1828, 384]) | item_emb_ngcf: torch.Size([10986, 384])\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"print(\"Done. user_emb_ngcf:\", user_emb_ngcf.shape,\n      \"| item_emb_ngcf:\", item_emb_ngcf.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:40:58.828334Z","iopub.execute_input":"2025-12-04T11:40:58.828874Z","iopub.status.idle":"2025-12-04T11:40:58.832837Z","shell.execute_reply.started":"2025-12-04T11:40:58.828850Z","shell.execute_reply":"2025-12-04T11:40:58.832038Z"}},"outputs":[{"name":"stdout","text":"Done. user_emb_ngcf: torch.Size([1828, 384]) | item_emb_ngcf: torch.Size([10986, 384])\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# CBFRS","metadata":{}},{"cell_type":"code","source":"# ==== CBF: Chuẩn bị content embedding cho item (từ CLIP) ====\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\n# item_feat: np.ndarray [num_items, feat_dim] đã build ở trên\n# has_emb:   np.ndarray [num_items] bool\n\nitem_feat_t = torch.from_numpy(item_feat).float().to(device)   # [num_items, D]\nitem_feat_t = F.normalize(item_feat_t, p=2, dim=1)             # đảm bảo L2-norm\n\nhas_emb_t = torch.from_numpy(has_emb).to(device)               # [num_items]\nprint(\"CBF item_feat_t:\", item_feat_t.shape, \"| has_emb_t:\", has_emb_t.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:41:06.357707Z","iopub.execute_input":"2025-12-04T11:41:06.358386Z","iopub.status.idle":"2025-12-04T11:41:06.370477Z","shell.execute_reply.started":"2025-12-04T11:41:06.358362Z","shell.execute_reply":"2025-12-04T11:41:06.369723Z"}},"outputs":[{"name":"stdout","text":"CBF item_feat_t: torch.Size([10986, 512]) | has_emb_t: torch.Size([10986])\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# ==== CBF: build user profile + recommend + evaluate (dùng chung train/test với CF) ====\n\ndef build_user_profile_cbf(u, user_train_items, item_feat_t, has_emb_t=None):\n    \"\"\"\n    Xây profile nội dung cho user u bằng trung bình embedding CLIP\n    của các item user đó đã tương tác (trong TRAIN).\n\n    u: user_id đã encode (0..num_users-1)\n    user_train_items: dict[u] -> list[item_id]\n    item_feat_t: torch.Tensor [num_items, D] (đã normalize)\n    has_emb_t: optional, torch.BoolTensor [num_items] (item nào có embedding)\n    \"\"\"\n    items = user_train_items.get(u, [])\n    if not items:\n        return None\n\n    items = torch.tensor(items, dtype=torch.long, device=item_feat_t.device)\n\n    if has_emb_t is not None:\n        mask = has_emb_t[items]\n        items = items[mask]\n        if items.numel() == 0:\n            return None\n\n    embs = item_feat_t[items]        # [n, D]\n    prof = embs.mean(dim=0)          # [D]\n    prof = F.normalize(prof, p=2, dim=0)\n    return prof\n\n\ndef recommend_cbf_for_user_cf_idx(\n    u,\n    user_train_items,\n    item_feat_t,\n    has_emb_t=None,\n    topk=10,\n    exclude_seen=True,\n):\n    \"\"\"\n    Recommend THUẦN CBF cho user u (ID đã encode).\n    Trả về list (item_id, score).\n    \"\"\"\n    user_prof = build_user_profile_cbf(u, user_train_items, item_feat_t, has_emb_t)\n    if user_prof is None:\n        return []\n\n    scores = item_feat_t @ user_prof   # [num_items], cosine vì đã normalize\n\n    # loại các item đã thấy trong train nếu cần\n    if exclude_seen:\n        seen = set(user_train_items.get(u, []))\n        if seen:\n            seen_idx = torch.tensor(list(seen), dtype=torch.long, device=item_feat_t.device)\n            scores[seen_idx] = -1e9\n\n    # nếu có has_emb_t, loại item không có embedding\n    if has_emb_t is not None:\n        scores[~has_emb_t] = -1e9\n\n    # top-k\n    topk = min(topk, scores.shape[0])\n    top_scores, top_idx = torch.topk(scores, k=topk)\n\n    recs = [(int(top_idx[i].item()), float(top_scores[i].item())) for i in range(topk)]\n    return recs\n\n\ndef evaluate_cbf_cf_idx(\n    item_feat_t,\n    user_train_items,\n    user_test_items,\n    has_emb_t=None,\n    k=10,\n):\n    \"\"\"\n    Đánh giá CBF-only với cùng split train/test như CF.\n    Trả về Precision@k, Recall@k, NDCG@k, HitRate@k.\n    \"\"\"\n    item_feat_n = F.normalize(item_feat_t, p=2, dim=1)\n\n    precisions, recalls, ndcgs, hit_rates = [], [], [], []\n\n    for u in user_test_items.keys():\n        test_items = set(user_test_items.get(u, []))\n        if not test_items:\n            continue\n\n        user_prof = build_user_profile_cbf(u, user_train_items, item_feat_n, has_emb_t)\n        if user_prof is None:\n            continue\n\n        scores = item_feat_n @ user_prof   # [num_items]\n\n        train_items = set(user_train_items.get(u, []))\n        if train_items:\n            train_idx = torch.tensor(list(train_items), dtype=torch.long, device=item_feat_t.device)\n            scores[train_idx] = -1e9\n\n        if has_emb_t is not None:\n            scores[~has_emb_t] = -1e9\n\n        top_k = torch.topk(scores, k=min(k, scores.shape[0])).indices.cpu().numpy()\n        hits = len(set(top_k) & test_items)\n\n        precision = hits / k\n        recall = hits / len(test_items)\n\n        # NDCG\n        dcg = 0.0\n        for rank, iid in enumerate(top_k):\n            if iid in test_items:\n                dcg += 1.0 / np.log2(rank + 2)\n        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(test_items), k)))\n        ndcg = dcg / idcg if idcg > 0 else 0.0\n\n        hit = 1.0 if hits > 0 else 0.0\n\n        precisions.append(precision)\n        recalls.append(recall)\n        ndcgs.append(ndcg)\n        hit_rates.append(hit)\n\n    return {\n        f\"Precision@{k}\": float(np.mean(precisions)) if precisions else 0.0,\n        f\"Recall@{k}\": float(np.mean(recalls)) if recalls else 0.0,\n        f\"NDCG@{k}\": float(np.mean(ndcgs)) if ndcgs else 0.0,\n        f\"HitRate@{k}\": float(np.mean(hit_rates)) if hit_rates else 0.0,\n        \"Users_eval\": int(len(precisions)),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:41:13.575021Z","iopub.execute_input":"2025-12-04T11:41:13.575283Z","iopub.status.idle":"2025-12-04T11:41:13.591704Z","shell.execute_reply.started":"2025-12-04T11:41:13.575264Z","shell.execute_reply":"2025-12-04T11:41:13.591131Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"metrics_cbf = evaluate_cbf_cf_idx(\n    item_feat_t,\n    user_train_items,\n    user_test_items,\n    has_emb_t=has_emb_t,\n    k=10\n)\nprint(\"CBF only:\", metrics_cbf)\n\nu_test = list(user_test_items.keys())[0]\nprint(\"Test user:\", u_test)\nprint(\"CBF recs:\", recommend_cbf_for_user_cf_idx(\n    u_test, user_train_items, item_feat_t, has_emb_t=has_emb_t, topk=5, exclude_seen=True\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:41:26.978010Z","iopub.execute_input":"2025-12-04T11:41:26.978589Z","iopub.status.idle":"2025-12-04T11:41:28.423820Z","shell.execute_reply.started":"2025-12-04T11:41:26.978565Z","shell.execute_reply":"2025-12-04T11:41:28.423051Z"}},"outputs":[{"name":"stdout","text":"CBF only: {'Precision@10': 0.002658486707566462, 'Recall@10': 0.008637337331332075, 'NDCG@10': 0.006343949110171408, 'HitRate@10': 0.02607361963190184, 'Users_eval': 1956}\nTest user: 706\nCBF recs: [(8280, 0.8371842503547668), (667, 0.8301780819892883), (1078, 0.8289486169815063), (6199, 0.8246238231658936), (10848, 0.8242899775505066)]\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# ==== Ensemble CF (NGCF) + CBF trên cùng split train/test ====\n\ndef evaluate_cf_cbf_ensemble(\n    user_emb_cf,          # user_emb_ngcf: [num_users, D_cf]\n    item_emb_cf,          # item_emb_ngcf: [num_items, D_cf]\n    item_feat_t,          # content embedding: [num_items, D_cb]\n    user_train_items,\n    user_test_items,\n    has_emb_t=None,\n    alpha_cf=0.7,         # trọng số CF\n    alpha_cb=0.3,         # trọng số CBF\n    k=10,\n):\n    # Chuẩn hóa embedding CF và CBF\n    U_cf = F.normalize(user_emb_cf, dim=1)        # [num_users, D_cf]\n    I_cf = F.normalize(item_emb_cf, dim=1)        # [num_items, D_cf]\n    I_cb = F.normalize(item_feat_t, dim=1)        # [num_items, D_cb]\n\n    precisions, recalls, ndcgs, hit_rates = [], [], [], []\n\n    for u in user_test_items.keys():\n        test_items = set(user_test_items.get(u, []))\n        if not test_items:\n            continue\n\n        # --- CF scores ---\n        scores_cf = (I_cf @ U_cf[u]).clone()      # [num_items]\n\n        # --- CBF scores ---\n        user_prof_cb = build_user_profile_cbf(u, user_train_items, I_cb, has_emb_t)\n        if user_prof_cb is None:\n            scores_cb = torch.zeros_like(scores_cf)\n        else:\n            scores_cb = I_cb @ user_prof_cb       # [num_items]\n\n        # --- Ensemble ---\n        scores = alpha_cf * scores_cf + alpha_cb * scores_cb\n\n        # loại item đã thấy trong train\n        train_items = set(user_train_items.get(u, []))\n        if train_items:\n            train_idx = torch.tensor(list(train_items), dtype=torch.long, device=item_feat_t.device)\n            scores[train_idx] = -1e9\n\n        if has_emb_t is not None:\n            scores[~has_emb_t] = -1e9\n\n        # top-k\n        top_k = torch.topk(scores, k=min(k, scores.shape[0])).indices.cpu().numpy()\n        hits = len(set(top_k) & test_items)\n\n        precision = hits / k\n        recall = hits / len(test_items)\n\n        # NDCG\n        dcg = 0.0\n        for rank, iid in enumerate(top_k):\n            if iid in test_items:\n                dcg += 1.0 / np.log2(rank + 2)\n        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(test_items), k)))\n        ndcg = dcg / idcg if idcg > 0 else 0.0\n\n        hit = 1.0 if hits > 0 else 0.0\n\n        precisions.append(precision)\n        recalls.append(recall)\n        ndcgs.append(ndcg)\n        hit_rates.append(hit)\n\n    return {\n        f\"Precision@{k}\": float(np.mean(precisions)) if precisions else 0.0,\n        f\"Recall@{k}\": float(np.mean(recalls)) if recalls else 0.0,\n        f\"NDCG@{k}\": float(np.mean(ndcgs)) if ndcgs else 0.0,\n        f\"HitRate@{k}\": float(np.mean(hit_rates)) if hit_rates else 0.0,\n        \"Users_eval\": int(len(precisions)),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:41:49.429812Z","iopub.execute_input":"2025-12-04T11:41:49.430365Z","iopub.status.idle":"2025-12-04T11:41:49.440239Z","shell.execute_reply.started":"2025-12-04T11:41:49.430341Z","shell.execute_reply":"2025-12-04T11:41:49.439564Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# CF-only\nmetrics_cf = evaluate_embeddings(\n    user_emb_ngcf, item_emb_ngcf,\n    user_train_items, user_test_items,\n    k=10\n)\nprint(\"NGCF only:\", metrics_cf)\n\n# CBF-only\nmetrics_cbf = evaluate_cbf_cf_idx(\n    item_feat_t,\n    user_train_items, user_test_items,\n    has_emb_t=has_emb_t,\n    k=10\n)\nprint(\"CBF only:\", metrics_cbf)\n\n# Ensemble CF + CBF\nmetrics_ens = evaluate_cf_cbf_ensemble(\n    user_emb_ngcf, item_emb_ngcf,\n    item_feat_t,\n    user_train_items, user_test_items,\n    has_emb_t=has_emb_t,\n    alpha_cf=0.7,  # có thể thử 0.6, 0.7, 0.8\n    alpha_cb=0.3,\n    k=10\n)\nprint(\"Ensemble CF+CBF:\", metrics_ens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T11:48:10.392495Z","iopub.execute_input":"2025-12-04T11:48:10.393059Z","iopub.status.idle":"2025-12-04T11:48:34.122669Z","shell.execute_reply.started":"2025-12-04T11:48:10.393037Z","shell.execute_reply":"2025-12-04T11:48:34.121980Z"}},"outputs":[{"name":"stdout","text":"NGCF only: {'Precision@10': 0.0020787746170678337, 'Recall@10': 0.003079139168671969, 'NDCG@10': 0.003288297209876962, 'HitRate@10': 0.019693654266958426, 'Users_eval': 1828}\nCBF only: {'Precision@10': 0.0026258205689277904, 'Recall@10': 0.007053956137902374, 'NDCG@10': 0.005932015630429195, 'HitRate@10': 0.025711159737417943, 'Users_eval': 1828}\nEnsemble CF+CBF: {'Precision@10': 0.0030634573304157554, 'Recall@10': 0.005868011641396695, 'NDCG@10': 0.004975186707981191, 'HitRate@10': 0.02899343544857768, 'Users_eval': 1828}\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"# Popularity: số lần xuất hiện trong A_train\nitem_pop = np.asarray(A_train.sum(axis=0)).flatten().astype(np.float32)\nif item_pop.max() > 0:\n    item_pop_norm = item_pop / (item_pop.max() + 1e-8)\nelse:\n    item_pop_norm = np.zeros_like(item_pop, dtype=np.float32)\n\nitem_pop_t = torch.from_numpy(item_pop_norm).float().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:06:15.108753Z","iopub.execute_input":"2025-12-04T12:06:15.109302Z","iopub.status.idle":"2025-12-04T12:06:15.120522Z","shell.execute_reply.started":"2025-12-04T12:06:15.109278Z","shell.execute_reply":"2025-12-04T12:06:15.119966Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"def recommend_popular(topk=10, exclude_items=None):\n    scores = item_pop_t.clone()\n    if exclude_items:\n        idx = torch.tensor(list(exclude_items), dtype=torch.long, device=item_pop_t.device)\n        scores[idx] = -1e9\n    k = min(topk, scores.shape[0])\n    top_scores, top_idx = torch.topk(scores, k=k)\n    return [(int(top_idx[i].item()), float(top_scores[i].item())) for i in range(k)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:06:23.887870Z","iopub.execute_input":"2025-12-04T12:06:23.888378Z","iopub.status.idle":"2025-12-04T12:06:23.892893Z","shell.execute_reply.started":"2025-12-04T12:06:23.888356Z","shell.execute_reply":"2025-12-04T12:06:23.892179Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"def recommend_for_user(\n    u,\n    user_train_items,\n    user_emb_cf,\n    item_emb_cf,\n    item_feat_t,\n    has_emb_t=None,\n    topk=10,\n    min_interactions_cf=3,\n):\n    \"\"\"\n    u: user_id đã encode (0..num_users-1) và đã đi qua remap kept_users\n    \"\"\"\n\n    # 1) Lấy history của user\n    history = user_train_items.get(u, [])\n    n_hist = len(history)\n\n    # Nếu user không có trong train (trong thực tế online gặp, trong offline ít gặp)\n    if n_hist == 0:\n        # cold user hoàn toàn: recommend theo POP\n        return recommend_popular(topk=topk, exclude_items=None)\n\n    # 2) Xác định trọng số CF / CBF tùy theo độ dày history\n    if n_hist < min_interactions_cf:\n        # lịch sử mỏng: ưu tiên CBF\n        alpha_cf = 0.3\n        alpha_cb = 0.7\n    else:\n        # lịch sử đủ dày: CF mạnh hơn\n        alpha_cf = 0.7\n        alpha_cb = 0.3\n\n    # 3) Chuẩn hóa embedding CF / CBF\n    U_cf = F.normalize(user_emb_cf, dim=1)\n    I_cf = F.normalize(item_emb_cf, dim=1)\n    I_cb = F.normalize(item_feat_t, p=2, dim=1)\n\n    # --- CF scores ---\n    scores_cf = (I_cf @ U_cf[u]).clone()  # [num_items]\n\n    # --- CBF scores ---\n    user_prof_cb = build_user_profile_cbf(u, user_train_items, I_cb, has_emb_t)\n    if user_prof_cb is None:\n        scores_cb = torch.zeros_like(scores_cf)\n    else:\n        scores_cb = I_cb @ user_prof_cb\n\n    # --- Ensemble CF + CBF ---\n    scores = alpha_cf * scores_cf + alpha_cb * scores_cb\n\n    # Mask các item đã xem\n    seen = set(history)\n    if seen:\n        seen_idx = torch.tensor(list(seen), dtype=torch.long, device=item_feat_t.device)\n        scores[seen_idx] = -1e9\n\n    # Không dùng item không có content nếu muốn\n    if has_emb_t is not None:\n        scores[~has_emb_t] = -1e9\n\n    # 4) Nếu user quá lạnh, có thể blend thêm POP\n    # (option): scores += gamma * item_pop_t\n    # Ví dụ cho user rất mới:\n    if n_hist < 2:\n        gamma = 0.2\n        scores = scores + gamma * item_pop_t\n\n    # top-k\n    k = min(topk, scores.shape[0])\n    top_scores, top_idx = torch.topk(scores, k=k)\n    recs = [(int(top_idx[i].item()), float(top_scores[i].item())) for i in range(k)]\n    return recs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:06:29.811296Z","iopub.execute_input":"2025-12-04T12:06:29.811567Z","iopub.status.idle":"2025-12-04T12:06:29.819746Z","shell.execute_reply.started":"2025-12-04T12:06:29.811547Z","shell.execute_reply":"2025-12-04T12:06:29.819011Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"import random\ni = random.randint(1, 1828)   \nu_test = list(user_test_items.keys())[i]\n\nrecs = recommend_for_user(\n    u_test,\n    user_train_items,\n    user_emb_ngcf,\n    item_emb_ngcf,\n    item_feat_t,\n    has_emb_t=has_emb_t,\n    topk=10,\n    min_interactions_cf=3,\n)\nprint(\"Hybrid recs for user\", u_test, \":\", recs[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:09:49.256213Z","iopub.execute_input":"2025-12-04T12:09:49.256925Z","iopub.status.idle":"2025-12-04T12:09:49.264308Z","shell.execute_reply.started":"2025-12-04T12:09:49.256895Z","shell.execute_reply":"2025-12-04T12:09:49.263601Z"}},"outputs":[{"name":"stdout","text":"Hybrid recs for user 524 : [(9230, 0.49658119678497314), (6189, 0.48777157068252563), (2181, 0.4794938564300537), (5430, 0.4740217924118042), (2944, 0.4693940579891205)]\n","output_type":"stream"}],"execution_count":78}]}