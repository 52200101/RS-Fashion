{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"},{"sourceId":9334353,"sourceType":"datasetVersion","datasetId":5546448,"isSourceIdPinned":false}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# import các thư viện cần thiết","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport time\nimport itertools\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"PyTorch:\", torch.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random, numpy as np, torch\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:00:21.172121Z","iopub.execute_input":"2025-12-11T10:00:21.172567Z","iopub.status.idle":"2025-12-11T10:00:21.270674Z","shell.execute_reply.started":"2025-12-11T10:00:21.172546Z","shell.execute_reply":"2025-12-11T10:00:21.270064Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import sys\nimport subprocess\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"fashion-clip\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:00:21.271481Z","iopub.execute_input":"2025-12-11T10:00:21.271752Z","iopub.status.idle":"2025-12-11T10:00:25.444682Z","shell.execute_reply.started":"2025-12-11T10:00:21.271723Z","shell.execute_reply":"2025-12-11T10:00:25.443762Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['/usr/bin/python3', '-m', 'pip', 'install', '-q', 'fashion-clip'], returncode=0)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from fashion_clip.fashion_clip import FashionCLIP\n\nfclip = FashionCLIP('fashion-clip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:00:25.446442Z","iopub.execute_input":"2025-12-11T10:00:25.446688Z","iopub.status.idle":"2025-12-11T10:00:41.412891Z","shell.execute_reply.started":"2025-12-11T10:00:25.446670Z","shell.execute_reply":"2025-12-11T10:00:41.412260Z"}},"outputs":[{"name":"stderr","text":"2025-12-11 10:00:31.596147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765447231.784950     362 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765447231.841821     362 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def build_text_column(outfits: pd.DataFrame, cfg):\n    text_cols = cfg.get(\"outfit_text_cols\", [])\n    def _build(row):\n        parts = []\n        for c in text_cols:\n            if c in row and pd.notna(row[c]):\n                parts.append(str(row[c]))\n        return \" \".join(parts)\n    return outfits.apply(_build, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:00:41.413668Z","iopub.execute_input":"2025-12-11T10:00:41.414234Z","iopub.status.idle":"2025-12-11T10:00:41.419675Z","shell.execute_reply.started":"2025-12-11T10:00:41.414189Z","shell.execute_reply":"2025-12-11T10:00:41.418878Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\n\ndef save_cf_model(model_name, model, user_emb, item_emb, extra_info=None):\n    \"\"\"\n    model_name: tên file, ví dụ 'ngcf', 'lightgcn', 'als'\n    model: mô hình PyTorch (NGCF / LightGCN) hoặc None (cho ALS)\n    user_emb, item_emb: torch.Tensor embedding\n    extra_info: bất kỳ thông tin nào muốn lưu thêm (dict)\n    \"\"\"\n    os.makedirs(\"saved_models\", exist_ok=True)\n    save_path = f\"saved_models/{model_name}.pt\"\n\n    save_dict = {\n        \"user_emb\": user_emb.cpu(),\n        \"item_emb\": item_emb.cpu(),\n        \"extra\": extra_info if extra_info is not None else {},\n    }\n\n    # Với NGCF / LightGCN thì có state_dict\n    if model is not None:\n        save_dict[\"model_state_dict\"] = model.state_dict()\n\n    torch.save(save_dict, save_path)\n    print(f\"[OK] Saved {model_name} → {save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_cf_model(model_name, ModelClass=None, device=\"cpu\"):\n    load_path = f\"saved_models/{model_name}.pt\"\n    ckpt = torch.load(load_path, map_location=device)\n\n    user_emb = ckpt[\"user_emb\"].to(device)\n    item_emb = ckpt[\"item_emb\"].to(device)\n    extra = ckpt[\"extra\"]\n\n    model = None\n    if ModelClass is not None:\n        # Detect model type automatically\n        if ModelClass.__name__.lower() == \"ngcf\":\n            model = ModelClass(\n                extra[\"num_users\"],\n                extra[\"num_items\"],\n                dim=extra[\"dim\"],\n                layers=extra[\"layers\"],\n            ).to(device)\n\n        elif ModelClass.__name__.lower() == \"lightgcn\":\n            model = ModelClass(\n                extra[\"num_users\"],\n                extra[\"num_items\"],\n                dim=extra[\"dim\"],\n                n_layers=extra[\"n_layers\"],   # <--- dùng đúng tên tham số\n            ).to(device)\n\n        else:\n            raise ValueError(\"ModelClass not supported.\")\n\n        model.load_state_dict(ckpt[\"model_state_dict\"])\n        model.eval()\n\n    print(f\"[OK] Loaded {model_name} from {load_path}\")\n    return model, user_emb, item_emb, extra\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:41:14.341761Z","iopub.execute_input":"2025-12-11T10:41:14.342557Z","iopub.status.idle":"2025-12-11T10:41:14.348518Z","shell.execute_reply.started":"2025-12-11T10:41:14.342528Z","shell.execute_reply":"2025-12-11T10:41:14.347665Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# Chuẩn bị dữ liệu H&M","metadata":{}},{"cell_type":"code","source":"import os, numpy as np, pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom fashion_clip.fashion_clip import FashionCLIP\n\n# =========================\n# 0) Helper functions\n# =========================\n# rule H&M: folder = 3 số đầu, filename = 10 số article_id.jpg\ndef article_id_to_image_path(article_id, image_root):\n    aid = str(article_id).zfill(10)\n    folder = aid[:3]\n    return os.path.join(image_root, folder, f\"{aid}.jpg\")\n\ndef load_dataset(base_path, cfg):\n    sep = cfg.get(\"csv_sep\", \",\")\n    outfits = pd.read_csv(\n        os.path.join(base_path, cfg[\"outfits_file\"]),\n        sep=sep,\n        quotechar='\"',\n        engine=\"python\",\n        on_bad_lines=\"skip\",\n        dtype={cfg[\"item_col\"]: str}   # ✅ ép article_id thành str\n    )\n    user_activity = pd.read_csv(\n        os.path.join(base_path, cfg[\"user_activity_file\"]),\n        sep=sep,\n        quotechar='\"',\n        engine=\"python\",\n        dtype={cfg[\"item_col\"]: str}\n    )\n    return outfits, user_activity\n\ndef build_group_images(outfits_df, cfg, image_dir):\n    if \"product_code\" not in outfits_df.columns:\n        raise ValueError(\"articles.csv không có cột product_code\")\n\n    records = []\n    for _, row in outfits_df.iterrows():\n        aid = row[cfg[\"item_col\"]]   # str\n        g   = row[\"product_code\"]\n        records.append({\n            \"product_code\": g,\n            \"article_id\": aid,\n            \"image_path\": article_id_to_image_path(aid, image_dir)\n        })\n    return pd.DataFrame(records)\n\n# =========================\n# 1) Config H&M\n# =========================\nDATASET = \"hm\"\nBASE_PATH = \"/kaggle/input/h-and-m-personalized-fashion-recommendations\"\nIMAGE_DIR = os.path.join(BASE_PATH, \"images\")\n\nEMB_DIR = \"/kaggle/working/embeddings\"\nos.makedirs(EMB_DIR, exist_ok=True)\n\nHM_TEXT_MAP_PATH  = os.path.join(EMB_DIR, f\"{DATASET}_text_emb_map.npy\")\nHM_IMAGE_GRP_PATH = os.path.join(EMB_DIR, f\"{DATASET}_image_emb_group.npy\")\nHM_FUSION_PATH    = os.path.join(EMB_DIR, f\"{DATASET}_fusion_emb.npy\")\n\nCFG = {\n    \"outfits_file\": \"articles.csv\",\n    \"user_activity_file\": \"transactions_train.csv\",\n    \"csv_sep\": \",\",\n\n    \"user_col\": \"customer_id\",\n    \"item_col\": \"article_id\",\n    \"start_time_col\": \"t_dat\",\n    \"end_time_col\": \"t_dat\",\n\n    \"outfit_text_cols\": [\n        \"prod_name\",\n        \"product_type_name\",\n        \"product_group_name\",\n        \"graphical_appearance_name\",\n        \"colour_group_name\",\n        \"perceived_colour_value_name\",\n        \"perceived_colour_master_name\",\n        \"department_name\",\n        \"index_name\",\n        \"index_group_name\",\n        \"section_name\",\n        \"garment_group_name\",\n        \"detail_desc\",\n    ],\n}\n\nprint(\"BASE_PATH:\", BASE_PATH)\nprint(\"IMAGE_DIR:\", IMAGE_DIR)\n\n# =========================\n# 2) Load data\n# =========================\noutfits, user_activity = load_dataset(BASE_PATH, CFG)\nprint(\"Outfits (articles):\", outfits.shape)\nprint(\"User activity:\", user_activity.shape)\n\n# =========================\n# 3) Build group_images (product_code -> list ảnh)\n# =========================\ngroup_images = build_group_images(outfits, CFG, IMAGE_DIR)\nprint(\"group_images:\", group_images.shape)\nprint(group_images.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from fashion_clip.fashion_clip import FashionCLIP\n# fclip = FashionCLIP('fashion-clip')\n\n# =========================\n# TEXT-ONLY EMBEDDING (H&M) - PREFIX hm_\n# =========================\n\nDATASET_NAME = \"hm\"  # prefix cho tất cả file H&M\n\nEMB_FILE_NAME = f\"{DATASET_NAME}_text_only_embeddings.npy\"\nOUTFIT_EMB_PATH = os.path.join(\"/kaggle/working\", EMB_FILE_NAME)\n\noutfit_embeddings = None\n\nif os.path.exists(OUTFIT_EMB_PATH):\n    print(f\"=> Đang load {DATASET_NAME} text-only embeddings từ\", OUTFIT_EMB_PATH)\n    arr = np.load(OUTFIT_EMB_PATH, allow_pickle=True)\n\n    if isinstance(arr, np.ndarray) and arr.dtype == object and arr.size == 1:\n        outfit_embeddings = arr.item()\n        print(\"  - Loaded dict, số article:\", len(outfit_embeddings))\n    else:\n        raise ValueError(\"Định dạng file embeddings không đúng (mong đợi dict).\")\n\nelse:\n    print(f\"=> Không tìm thấy {EMB_FILE_NAME} → tính TEXT embedding cho toàn bộ articles...\")\n\n    outfits[\"__text_for_clip\"] = build_text_column(outfits, CFG)\n    texts = outfits[\"__text_for_clip\"].fillna(\"\").tolist()\n    article_ids = outfits[\"article_id\"].astype(str).values\n\n    text_embeddings = fclip.encode_text(texts, batch_size=32)\n    outfit_embeddings = dict(zip(article_ids, text_embeddings))\n\n    print(\"=> Lưu text-only embeddings vào\", OUTFIT_EMB_PATH)\n    np.save(OUTFIT_EMB_PATH, outfit_embeddings)\n\nprint(\"Số article có embedding:\", len(outfit_embeddings))\nfeat_dim = next(iter(outfit_embeddings.values())).shape[0]\nprint(\"Kích thước embedding:\", feat_dim)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom glob import glob\n\nIMAGE_DIR = \"/kaggle/input/h-and-m-personalized-fashion-recommendations/images\"\n\n# Cách 1: đếm tất cả file .jpg trong toàn bộ thư mục con\nall_imgs = glob(os.path.join(IMAGE_DIR, \"*\", \"*.jpg\"))\nprint(\"Tổng số ảnh (.jpg):\", len(all_imgs))\n\n# (tuỳ chọn) đếm theo từng folder 3 số đầu\nfolder_counts = {}\nfor folder in sorted(os.listdir(IMAGE_DIR)):\n    fpath = os.path.join(IMAGE_DIR, folder)\n    if os.path.isdir(fpath):\n        folder_counts[folder] = len(glob(os.path.join(fpath, \"*.jpg\")))\n\nprint(\"Số folder:\", len(folder_counts))\nprint(\"3 folder nhiều ảnh nhất:\")\nfor k in sorted(folder_counts, key=folder_counts.get, reverse=True)[:3]:\n    print(k, folder_counts[k])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chuẩn bị dữ liệu vibrent-clothes-rental-dataset","metadata":{}},{"cell_type":"code","source":"# ========= CONFIG DATASET =========\n# Chỉ cần sửa block này khi đổi dataset\n\nDATASET_NAME = \"vibrent\"  # tên để đặt file output embeddings, log,...\n\n# local:\n# BASE_PATH = r\"D:\\DACNTT\"\n# IMAGE_DIR = os.path.join(BASE_PATH, \"images\")\n\n# Kaggle:\nBASE_PATH = \"/kaggle/input/vibrent-clothes-rental-dataset\"\nIMAGE_DIR = os.path.join(BASE_PATH, \"images\")\n# File & cột tương ứng với dataset Vibrent\nCFG = {\n    \"outfits_file\": \"outfits.csv\",\n    \"pictures_file\": \"picture_triplets.csv\",\n    \"user_activity_file\": \"user_activity_triplets.csv\",\n    \"csv_sep\": \";\",\n    # cột user / item / thời gian trong user_activity\n    \"user_col\": \"customer.id\",\n    \"item_col\": \"outfit.id\",\n    \"start_time_col\": \"rentalPeriod.start\",\n    \"end_time_col\": \"rentalPeriod.end\",\n    # cột trong pictures\n    \"picture_outfit_col\": \"outfit.id\",\n    \"picture_filename_col\": \"file_name\",\n    \"picture_displayorder_col\": \"displayOrder\",\n    # cột để xây text cho FashionCLIP\n    # sẽ join các cột này bằng khoảng trắng (bỏ NA)\n    \"outfit_text_cols\": [\"description\", \"outfit_tags\"],\n}\n\nprint(\"BASE_PATH:\", BASE_PATH)\nprint(\"IMAGE_DIR:\", IMAGE_DIR)\nprint(\"CFG:\", CFG)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:01:32.331910Z","iopub.execute_input":"2025-12-11T10:01:32.332606Z","iopub.status.idle":"2025-12-11T10:01:32.338847Z","shell.execute_reply.started":"2025-12-11T10:01:32.332562Z","shell.execute_reply":"2025-12-11T10:01:32.338078Z"}},"outputs":[{"name":"stdout","text":"BASE_PATH: /kaggle/input/vibrent-clothes-rental-dataset\nIMAGE_DIR: /kaggle/input/vibrent-clothes-rental-dataset/images\nCFG: {'outfits_file': 'outfits.csv', 'pictures_file': 'picture_triplets.csv', 'user_activity_file': 'user_activity_triplets.csv', 'csv_sep': ';', 'user_col': 'customer.id', 'item_col': 'outfit.id', 'start_time_col': 'rentalPeriod.start', 'end_time_col': 'rentalPeriod.end', 'picture_outfit_col': 'outfit.id', 'picture_filename_col': 'file_name', 'picture_displayorder_col': 'displayOrder', 'outfit_text_cols': ['description', 'outfit_tags']}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"load dữ liệu","metadata":{}},{"cell_type":"code","source":"def load_dataset(base_path, cfg):\n    sep = cfg.get(\"csv_sep\", \",\")\n\n    outfits = pd.read_csv(\n        os.path.join(base_path, cfg[\"outfits_file\"]),\n        sep=sep,\n        quotechar='\"',\n        engine=\"python\",\n        on_bad_lines=\"skip\"\n    )\n\n    pictures = pd.read_csv(\n        os.path.join(base_path, cfg[\"pictures_file\"]),\n        sep=sep,\n        quotechar='\"',\n        engine=\"python\"\n    )\n\n    user_activity = pd.read_csv(\n        os.path.join(base_path, cfg[\"user_activity_file\"]),\n        sep=sep,\n        quotechar='\"',\n        engine=\"python\"\n    )\n\n    print(\"Outfits:\", outfits.shape)\n    print(\"Pictures:\", pictures.shape)\n    print(\"User activity:\", user_activity.shape)\n    return outfits, pictures, user_activity\n\n\noutfits, pictures, user_activity = load_dataset(BASE_PATH, CFG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:01:34.151720Z","iopub.execute_input":"2025-12-11T10:01:34.152519Z","iopub.status.idle":"2025-12-11T10:01:35.091484Z","shell.execute_reply.started":"2025-12-11T10:01:34.152488Z","shell.execute_reply":"2025-12-11T10:01:35.090660Z"}},"outputs":[{"name":"stdout","text":"Outfits: (15649, 11)\nPictures: (50193, 4)\nUser activity: (64419, 4)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"image_dir = \"/kaggle/input/vibrent-clothes-rental-dataset/images\"\n\ndef load_image(file_name):\n    path = os.path.join(image_dir, file_name)\n    if os.path.exists(path):\n        try:\n            return Image.open(path)\n        except:\n            return None\n    return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:20:02.463626Z","iopub.execute_input":"2025-12-11T10:20:02.464399Z","iopub.status.idle":"2025-12-11T10:20:02.468683Z","shell.execute_reply.started":"2025-12-11T10:20:02.464369Z","shell.execute_reply":"2025-12-11T10:20:02.467946Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"main_pictures = (\n    pictures.sort_values(\"displayOrder\")\n    .groupby(\"outfit.id\")\n    .first() \n    .reset_index()\n)\n\nmain_pictures[\"image_obj\"] = main_pictures[\"file_name\"].apply(load_image)\nmain_pictures = main_pictures[main_pictures[\"image_obj\"].notnull()]\nprint(\"Số outfit có main picture (theo CSV):\", len(main_pictures))\nprint(main_pictures.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:20:35.598629Z","iopub.execute_input":"2025-12-11T10:20:35.599163Z","iopub.status.idle":"2025-12-11T10:22:43.930730Z","shell.execute_reply.started":"2025-12-11T10:20:35.599131Z","shell.execute_reply":"2025-12-11T10:22:43.929963Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/4 [14:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Số outfit có main picture (theo CSV): 15157\n                                 outfit.id  \\\n0  outfit.00004b4d01ca4ab0a70cf073ba74fefa   \n1  outfit.0013691ff35b440e9dcfe1748ec184c7   \n2  outfit.0014a5c89b244077a3d7cffd4549718e   \n3  outfit.0018701ce6b049ebadc314d16623caa8   \n4  outfit.001bf665330140cf854dcfb1cbff6b5f   \n\n                                 picture.id  displayOrder  \\\n0  picture.a2b794c7ef83495a8997e7b0c318d65a             1   \n1  picture.9c821ecbecb14c959f35078010fb91f3             1   \n2  picture.b9aa39eb40f5410fa4fe101236241b19             1   \n3  picture.b944a50f20fd4c7f954213dc7c38a776             1   \n4  picture.fb1ff67a0bbc418b88ebb5560fac88a1             0   \n\n                              file_name  \\\n0  a2b794c7ef83495a8997e7b0c318d65a.jpg   \n1  9c821ecbecb14c959f35078010fb91f3.jpg   \n2  b9aa39eb40f5410fa4fe101236241b19.jpg   \n3  b944a50f20fd4c7f954213dc7c38a776.jpg   \n4  fb1ff67a0bbc418b88ebb5560fac88a1.jpg   \n\n                                           image_obj  \n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...  \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...  \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...  \n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...  \n4  <PIL.JpegImagePlugin.JpegImageFile image mode=...  \n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Tạo outfitsEmbedding hoặc load lên nếu đã có","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:37:20.533451Z","iopub.execute_input":"2025-12-11T10:37:20.534046Z","iopub.status.idle":"2025-12-11T10:37:20.538314Z","shell.execute_reply.started":"2025-12-11T10:37:20.534015Z","shell.execute_reply":"2025-12-11T10:37:20.537400Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nEMB_FILE_NAME = \"outfit_embeddings.pkl\"\nOUTFIT_EMB_PATH = os.path.join(\"/kaggle/working/\", EMB_FILE_NAME)\n\noutfit_embeddings = None\n\n\ndef build_text_column(outfits: pd.DataFrame, cfg):\n    text_cols = cfg.get(\"outfit_text_cols\", [])\n    def _build(row):\n        parts = []\n        for c in text_cols:\n            if c in row and pd.notna(row[c]):\n                parts.append(str(row[c]))\n        return \" \".join(parts)\n    return outfits.apply(_build, axis=1)\n\n\n# ============================================================\n# 1) LOAD EMBEDDING NẾU ĐÃ CÓ\n# ============================================================\nif os.path.exists(OUTFIT_EMB_PATH):\n    print(\"=> Đang load outfit_embeddings từ\", OUTFIT_EMB_PATH)\n\n    with open(OUTFIT_EMB_PATH, \"rb\") as f:\n        outfit_embeddings = pickle.load(f)\n\n    print(\"  - Loaded dict, số outfit:\", len(outfit_embeddings))\n\nelse:\n    # ============================================================\n    # 2) TÍNH TEXT + IMAGE EMBEDDING\n    # ============================================================\n    print(\"=> Không thấy embedding → bắt đầu tính TEXT + IMAGE bằng FashionCLIP\")\n\n    # ===== TEXT EMBEDDINGS =====\n    print(\"[1/2] Encoding TEXT ...\")\n    outfits[\"text\"] = build_text_column(outfits, CFG)\n    text_embeddings = fclip.encode_text(outfits[\"text\"].tolist(), batch_size=32)\n\n    # ===== IMAGE EMBEDDINGS =====\n    print(\"[2/2] Encoding IMAGES ...\")\n    images = main_pictures[\"image_obj\"].tolist()\n    image_embeddings = fclip.encode_images(images, batch_size=32)\n    print(\"Ảnh encode được:\", len(image_embeddings))\n\n    # ===== GỘP TEXT + IMAGE =====\n    text_emb_map = dict(zip(outfits[\"id\"], text_embeddings))\n    image_emb_map = dict(zip(main_pictures[\"outfit.id\"], image_embeddings))\n\n    outfit_embeddings = {}\n\n    for oid in outfits[\"id\"]:\n        if oid in text_emb_map and oid in image_emb_map:\n            t = text_emb_map[oid]\n            i = image_emb_map[oid]\n\n            # Chuẩn hóa trước khi cộng\n            t = t / np.linalg.norm(t)\n            i = i / np.linalg.norm(i)\n\n            outfit_embeddings[oid] = 0.5 * t + 0.5 * i\n\n    # ===== SAVE =====\n    print(\"=> Saving:\", OUTFIT_EMB_PATH)\n    with open(OUTFIT_EMB_PATH, \"wb\") as f:\n        pickle.dump(outfit_embeddings, f)\n\n# ============================================================\n# 3) THÔNG TIN EMBEDDING\n# ============================================================\nprint(\"Done. Outfit embeddings =\", len(outfit_embeddings))\nfeat_dim = next(iter(outfit_embeddings.values())).shape[0]\nprint(\"Embedding dimension =\", feat_dim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:38:54.190475Z","iopub.execute_input":"2025-12-11T10:38:54.191324Z","iopub.status.idle":"2025-12-11T10:38:54.250662Z","shell.execute_reply.started":"2025-12-11T10:38:54.191291Z","shell.execute_reply":"2025-12-11T10:38:54.249948Z"}},"outputs":[{"name":"stdout","text":"=> Đang load outfit_embeddings từ /kaggle/working/outfit_embeddings.pkl\n  - Loaded dict, số outfit: 15157\nDone. Outfit embeddings = 15157\nEmbedding dimension = 512\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Hàm lấy outfits từ text ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef find_outfits_from_text(query_text, outfit_embeddings, model, topk=10):\n    \"\"\"\n    Tìm top-K outfit phù hợp với text (query) dựa trên FashionCLIP embedding.\n    \"\"\"\n    # 1) Encode text thành embedding\n    text_emb = model.encode_text([query_text], batch_size=1)[0]\n    text_emb = text_emb.astype(np.float32)\n\n    # L2 normalize để cosine similarity\n    text_emb = text_emb / (np.linalg.norm(text_emb) + 1e-8)\n\n    # 2) Chuẩn bị item embedding matrix\n    outfit_ids = list(outfit_embeddings.keys())\n    item_matrix = np.vstack([outfit_embeddings[oid] for oid in outfit_ids]).astype(np.float32)\n\n    # Normalize item embeddings\n    item_matrix = item_matrix / (np.linalg.norm(item_matrix, axis=1, keepdims=True) + 1e-8)\n\n    # 3) Tính cosine similarity\n    scores = item_matrix @ text_emb     # shape = [num_items]\n\n    # 4) Lấy top-k\n    top_idx = np.argsort(-scores)[:topk]\n\n    results = [(outfit_ids[i], float(scores[i])) for i in top_idx]\n    return results\ndef get_images_for_query(query_text, topk=5):\n    results = find_outfits_from_text(query_text, outfit_embeddings, fclip, topk=topk)\n\n    pics = []\n    for oid, score in results:\n        row = main_pictures[main_pictures[\"outfit.id\"] == oid]\n        if len(row) > 0:\n            file = row.iloc[0][\"file_name\"]\n            img = load_image(file)\n            pics.append((img, oid, score))\n    return pics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:39:45.409894Z","iopub.execute_input":"2025-12-11T10:39:45.410221Z","iopub.status.idle":"2025-12-11T10:39:45.417332Z","shell.execute_reply.started":"2025-12-11T10:39:45.410183Z","shell.execute_reply":"2025-12-11T10:39:45.416554Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from IPython.display import display\n\nquery = \"red dress\"\npics = get_images_for_query(query, topk=5)\n\nfor img, oid, score in pics:\n    print(f\"Outfit: {oid} | score={score:.3f}\")\n    display(img)\n","metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BERT+ImageEmbedding có sẵn","metadata":{}},{"cell_type":"code","source":"# import sys\n# import subprocess\n# subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from transformers import BertTokenizer, BertModel\n# import torch\n# from tqdm import tqdm\n\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n# bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n# bert_model.eval()\n\n# def encode_texts_bert(texts, batch_size=16):\n#     embeddings = []\n#     for i in range(0, len(texts), batch_size):\n#         batch = texts[i:i+batch_size]\n#         tokens = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=64).to(device)\n#         with torch.no_grad():\n#             outputs = bert_model(**tokens)\n#             cls_emb = outputs.pooler_output  # vector [CLS]\n#             cls_emb = torch.nn.functional.normalize(cls_emb, p=2, dim=1)\n#             embeddings.append(cls_emb.cpu().numpy())\n#     return np.vstack(embeddings)\n\n# # === Tạo embedding văn bản bằng BERT ===\n# texts = outfits[\"text\"].fillna(\"\").tolist()\n# bert_text_embeddings = encode_texts_bert(texts, batch_size=16)\n# print(\"BERT embedding shape:\", bert_text_embeddings.shape)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# embedding_dir = os.path.join(base_path, \"embeddings\", \"EfficientNet_V2_L_final\")\n# dataset_image_embeddings = {}\n# for _, row in main_pictures.iterrows():\n#     picture_id = row[\"picture.id\"]\n#     file_path = os.path.join(embedding_dir, f\"picture.{picture_id}.npy\")\n#     if os.path.exists(file_path):\n#         try:\n#             vec = np.load(file_path)\n#             dataset_image_embeddings[row[\"outfit.id\"]] = vec\n#         except Exception as e:\n#             print(f\"Lỗi load {file_path}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CFRS ","metadata":{}},{"cell_type":"code","source":"#H&M\n#CF preprocessing – encode user/item, split train/test, build A_train\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nu_col = CFG[\"user_col\"]\ni_col = CFG[\"item_col\"]\nt_start = CFG[\"start_time_col\"]\nt_end = CFG[\"end_time_col\"]\n\n# 1) Parse thời gian & sort\nfor col in [t_start, t_end]:\n    user_activity[col] = pd.to_datetime(user_activity[col], errors=\"coerce\", utc=True)\nuser_activity = user_activity.dropna(subset=[t_start, t_end])\nuser_activity = user_activity.sort_values([u_col, t_start, t_end])\nprint(\"[OK] Parse & sort thời gian.\")\n\n# === CẮT TOP 10% USER HOẠT ĐỘNG NHẤT ===\nuser_counts = user_activity[u_col].value_counts()\nratio = 0.10\nn_users_keep = max(1, int(len(user_counts) * ratio))\nkeep_users = user_counts.head(n_users_keep).index\n\nuser_activity = user_activity[user_activity[u_col].isin(keep_users)].copy()\n\nprint(\"[AFTER CUT 10%] users =\", user_activity[u_col].nunique(),\n      \"| items =\", user_activity[i_col].nunique(),\n      \"| interactions =\", len(user_activity))\n\n# 2) Encode user / item\nuser_enc = LabelEncoder().fit(user_activity[u_col].astype(str))\nitem_enc = LabelEncoder().fit(user_activity[i_col].astype(str))\n\nuser_activity[\"user_id\"] = user_enc.transform(user_activity[u_col].astype(str))\nuser_activity[\"item_id\"] = item_enc.transform(user_activity[i_col].astype(str))\n\nnum_users = user_activity[\"user_id\"].nunique()\nnum_items = user_activity[\"item_id\"].nunique()\n\n# mapping giữa item_id (index trong matrix) và outfit-id gốc\noid_from_item = dict(zip(user_activity[\"item_id\"], user_activity[i_col]))\nitem_from_oid = {v: k for k, v in oid_from_item.items()}\n\nprint(f\"[COUNTS] users={num_users} | items={num_items} | interactions={len(user_activity)}\")\n\n# 3) Chia train/test theo thời gian cho từng user\nTEST_RATIO = 0.2\nuser_train_items, user_test_items = {}, {}\n\nfor u, g in user_activity.groupby(\"user_id\", sort=False):\n    g = g.sort_values([t_start, t_end])\n    items = list(g[\"item_id\"])\n    if len(items) < 3:\n        continue\n    test_size = max(1, int(len(items) * TEST_RATIO))\n    user_train_items[u] = items[:-test_size]\n    user_test_items[u] = items[-test_size:]\n\nprint(f\"[SPLIT] users trong split = {len(user_train_items)}\")\n\n# Loại user không còn test hợp lệ\ntrain_item_set = set(i for items in user_train_items.values() for i in items)\nfor u in list(user_test_items.keys()):\n    filtered = [i for i in user_test_items[u] if i in train_item_set]\n    if len(filtered) == 0:\n        user_test_items.pop(u, None)\n        user_train_items.pop(u, None)\n    else:\n        user_test_items[u] = filtered\n\nprint(f\"[SPLIT after filter] users = {len(user_train_items)}\")\n\n# 4) Build A_train (CSR)\nrows, cols = [], []\nfor u, items in user_train_items.items():\n    rows.extend([u] * len(items))\n    cols.extend(items)\n\nif not rows:\n    raise RuntimeError(\"Không còn tương tác train nào sau khi split.\")\n\nA_train = csr_matrix(\n    (np.ones(len(rows), dtype=np.float32),\n     (np.array(rows), np.array(cols))),\n    shape=(num_users, num_items),\n    dtype=np.float32\n)\nA_train.data[:] = 1.0\nA_train.eliminate_zeros()\n\nprint(f\"[MATRIX] A_train shape={A_train.shape} | nnz={A_train.nnz}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#vibrent\n#CF preprocessing – encode user/item, split train/test, build A_train\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nu_col = CFG[\"user_col\"]\ni_col = CFG[\"item_col\"]\nt_start = CFG[\"start_time_col\"]\nt_end = CFG[\"end_time_col\"]\n\n# 1) Parse thời gian & sort\nfor col in [t_start, t_end]:\n    user_activity[col] = pd.to_datetime(user_activity[col], errors=\"coerce\", utc=True)\nuser_activity = user_activity.dropna(subset=[t_start, t_end])\nuser_activity = user_activity.sort_values([u_col, t_start, t_end])\nprint(\"[OK] Parse & sort thời gian.\")\n\n# 2) Encode user / item\nuser_enc = LabelEncoder().fit(user_activity[u_col].astype(str))\nitem_enc = LabelEncoder().fit(user_activity[i_col].astype(str))\n\nuser_activity[\"user_id\"] = user_enc.transform(user_activity[u_col].astype(str))\nuser_activity[\"item_id\"] = item_enc.transform(user_activity[i_col].astype(str))\n\nnum_users = user_activity[\"user_id\"].nunique()\nnum_items = user_activity[\"item_id\"].nunique()\n\n# mapping giữa item_id (index trong matrix) và outfit-id gốc\noid_from_item = dict(zip(user_activity[\"item_id\"], user_activity[i_col]))\nitem_from_oid = {v: k for k, v in oid_from_item.items()}\n\nprint(f\"[COUNTS] users={num_users} | items={num_items} | interactions={len(user_activity)}\")\n\n# 3) Chia train/test theo thời gian cho từng user\nTEST_RATIO = 0.2\nuser_train_items, user_test_items = {}, {}\n\nfor u, g in user_activity.groupby(\"user_id\", sort=False):\n    g = g.sort_values([t_start, t_end])\n    items = list(g[\"item_id\"])\n    if len(items) < 3:\n        continue\n    test_size = max(1, int(len(items) * TEST_RATIO))\n    user_train_items[u] = items[:-test_size]\n    user_test_items[u] = items[-test_size:]\n\nprint(f\"[SPLIT] users trong split = {len(user_train_items)}\")\n\n# Loại user không còn test hợp lệ\ntrain_item_set = set(i for items in user_train_items.values() for i in items)\nfor u in list(user_test_items.keys()):\n    filtered = [i for i in user_test_items[u] if i in train_item_set]\n    if len(filtered) == 0:\n        user_test_items.pop(u, None)\n        user_train_items.pop(u, None)\n    else:\n        user_test_items[u] = filtered\n\nprint(f\"[SPLIT after filter] users = {len(user_train_items)}\")\n\n# 4) Build A_train (CSR)\nrows, cols = [], []\nfor u, items in user_train_items.items():\n    rows.extend([u] * len(items))\n    cols.extend(items)\n\nif not rows:\n    raise RuntimeError(\"Không còn tương tác train nào sau khi split.\")\n\nA_train = csr_matrix(\n    (np.ones(len(rows), dtype=np.float32),\n     (np.array(rows), np.array(cols))),\n    shape=(num_users, num_items),\n    dtype=np.float32\n)\nA_train.data[:] = 1.0\nA_train.eliminate_zeros()\n\nprint(f\"[MATRIX] A_train shape={A_train.shape} | nnz={A_train.nnz}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:41:35.358518Z","iopub.execute_input":"2025-12-11T10:41:35.359245Z","iopub.status.idle":"2025-12-11T10:41:37.938565Z","shell.execute_reply.started":"2025-12-11T10:41:35.359197Z","shell.execute_reply":"2025-12-11T10:41:37.937770Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n[OK] Parse & sort thời gian.\n[COUNTS] users=2293 | items=10986 | interactions=64419\n[SPLIT] users trong split = 1979\n[SPLIT after filter] users = 1956\n[MATRIX] A_train shape=(2293, 10986) | nnz=47356\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"#Các hàm cần thiết\nclass BPRLoss(nn.Module):\n    def __init__(self, reg_lambda=1e-4):\n        super().__init__()\n        self.reg_lambda = reg_lambda\n\n    def forward(self, user_emb, pos_emb, neg_emb):\n        pos_score = torch.sum(user_emb * pos_emb, dim=1)\n        neg_score = torch.sum(user_emb * neg_emb, dim=1)\n        diff = pos_score - neg_score\n        bpr = F.softplus(-diff).mean()\n        reg = (\n            user_emb.norm(2).pow(2)\n            + pos_emb.norm(2).pow(2)\n            + neg_emb.norm(2).pow(2)\n        ) / user_emb.size(0)\n        return bpr + self.reg_lambda * reg\n\n\ndef l2n_t(x, eps=1e-8):\n    return x / (x.norm(dim=1, keepdim=True) + eps)\n\n\ndef sample_triplets(user_train_items, num_items, n_samples_per_user=5):\n    users, pos_items, neg_items = [], [], []\n    for u, pos_list in user_train_items.items():\n        if not pos_list:\n            continue\n        pos_set = set(pos_list)\n        for _ in range(n_samples_per_user):\n            p = random.choice(pos_list)\n            n = np.random.randint(0, num_items)\n            while n in pos_set:\n                n = np.random.randint(0, num_items)\n            users.append(u)\n            pos_items.append(p)\n            neg_items.append(n)\n    if not users:\n        users, pos_items, neg_items = [0], [0], [1]\n    return (\n        torch.tensor(users, dtype=torch.long),\n        torch.tensor(pos_items, dtype=torch.long),\n        torch.tensor(neg_items, dtype=torch.long),\n    )\n\n\n@torch.no_grad()\ndef build_item_neighbors(item_emb, topk=100, batch=2048):\n    item_emb = l2n_t(item_emb)\n    N, _ = item_emb.shape\n    all_topk = []\n    for i0 in range(0, N, batch):\n        xb = item_emb[i0 : i0 + batch]\n        scores = xb @ item_emb.T\n        for b in range(scores.shape[0]):\n            idx = i0 + b\n            if idx < N:\n                scores[b, idx] = -1e9\n        _, idxs = torch.topk(scores, k=min(topk, N - 1), dim=1)\n        all_topk.append(idxs.cpu().numpy())\n    return np.vstack(all_topk)\n\n\ndef hard_negative_sampling_from_neighbors(user_train_items, neighbors_idx, n_samples_per_user=5):\n    users, pos_items, neg_items = [], [], []\n    for u, pos_list in user_train_items.items():\n        if not pos_list:\n            continue\n        pos_set = set(pos_list)\n        for _ in range(n_samples_per_user):\n            p = random.choice(pos_list)\n            cand = neighbors_idx[p]\n            cand = [c for c in cand if c not in pos_set]\n            if not cand:\n                continue\n            n = random.choice(cand)\n            users.append(u)\n            pos_items.append(p)\n            neg_items.append(n)\n    if not users:\n        return sample_triplets(user_train_items, num_items, n_samples_per_user)\n    return (\n        torch.tensor(users, dtype=torch.long),\n        torch.tensor(pos_items, dtype=torch.long),\n        torch.tensor(neg_items, dtype=torch.long),\n    )\n\n\ndef mixed_negative_sampling(user_train_items, num_items, item_emb=None, neighbors_idx=None,\n                            n_samples_per_user=10, mix=0.5):\n    if (item_emb is not None) and (neighbors_idx is not None) and (np.random.rand() < mix):\n        return hard_negative_sampling_from_neighbors(user_train_items, neighbors_idx, n_samples_per_user)\n    else:\n        return sample_triplets(user_train_items, num_items, n_samples_per_user)\n\n\ndef build_norm_adj_sparse(A_csr: csr_matrix) -> sp.csr_matrix:\n    n_users, n_items = A_csr.shape\n    R = A_csr.tocsr()\n    upper = sp.hstack([sp.csr_matrix((n_users, n_users), dtype=np.float32), R], format=\"csr\")\n    lower = sp.hstack([R.T, sp.csr_matrix((n_items, n_items), dtype=np.float32)], format=\"csr\")\n    adj = sp.vstack([upper, lower], format=\"csr\").astype(np.float32)\n\n    deg = np.array(adj.sum(axis=1)).flatten().astype(np.float32)\n    deg_inv_sqrt = np.zeros_like(deg, dtype=np.float32)\n    mask = deg > 0\n    deg_inv_sqrt[mask] = np.power(deg[mask], -0.5, dtype=np.float32)\n    D_inv_sqrt = sp.diags(deg_inv_sqrt, format=\"csr\", dtype=np.float32)\n\n    norm_adj = D_inv_sqrt @ adj @ D_inv_sqrt\n    return norm_adj.tocsr()\n\n\ndef scipy_to_torch_sparse(mat: sp.csr_matrix) -> torch.Tensor:\n    coo = mat.tocoo()\n    idx = np.vstack([coo.row, coo.col]).astype(np.int64)\n    indices = torch.from_numpy(idx)\n    values = torch.from_numpy(coo.data.astype(np.float32))\n    return torch.sparse_coo_tensor(indices, values, coo.shape).coalesce()\n\n\ndef evaluate_embeddings(user_emb, item_emb, user_train_items, user_test_items, k=10):\n    U = l2n_t(user_emb.cpu())\n    I = l2n_t(item_emb.cpu())\n    I_np = I.numpy()\n\n    precisions, recalls, ndcgs, hit_rates = [], [], [], []\n\n    for u in user_test_items.keys():\n        train_items = set(user_train_items.get(u, []))\n        test_items = set(user_test_items.get(u, []))\n        if not test_items:\n            continue\n\n        scores = cosine_similarity(U[u].unsqueeze(0).numpy(), I_np)[0]\n        # if train_items:\n        #     scores[list(train_items)] = -1e9\n\n        top_k = np.argpartition(scores, -k)[-k:]\n        top_k = top_k[np.argsort(scores[top_k])[::-1]]\n\n        hits = len(set(top_k) & test_items)\n\n        # ---- Recall/Precision như cũ ----\n        precision = hits / k\n        recall = hits / len(test_items)\n\n        # ---- NDCG như cũ ----\n        dcg = sum(1 / np.log2(i + 2) for i, iid in enumerate(top_k) if iid in test_items)\n        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(test_items), k)))\n        ndcg = dcg / idcg if idcg > 0 else 0\n\n        # ---- HitRate@K = 1 nếu có ít nhất 1 hit ----\n        hit = 1.0 if hits > 0 else 0.0\n\n        precisions.append(precision)\n        recalls.append(recall)\n        ndcgs.append(ndcg)\n        hit_rates.append(hit)\n\n    return {\n        f\"Precision@{k}\": float(np.mean(precisions)),\n        f\"Recall@{k}\": float(np.mean(recalls)),\n        f\"NDCG@{k}\": float(np.mean(ndcgs)),\n        f\"HitRate@{k}\": float(np.mean(hit_rates)),\n        \"Users_eval\": int(len(precisions)),\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:41:37.939927Z","iopub.execute_input":"2025-12-11T10:41:37.940161Z","iopub.status.idle":"2025-12-11T10:41:37.962177Z","shell.execute_reply.started":"2025-12-11T10:41:37.940144Z","shell.execute_reply":"2025-12-11T10:41:37.961588Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# Item–item similarity từ FashionCLIP + build norm_adj_dev\nfeat_dim = next(iter(outfit_embeddings.values())).shape[0]\nitem_feat = np.zeros((num_items, feat_dim), dtype=np.float32)\nhas_emb = np.zeros(num_items, dtype=bool)\n\nfor item_id, oid in oid_from_item.items():\n    v = outfit_embeddings.get(oid)\n    if v is not None:\n        item_feat[item_id] = v.astype(np.float32)\n        has_emb[item_id] = True\n\nprint(\"Số item có embedding CLIP:\", has_emb.sum(), \"/\", num_items)\n\nitem_feat = item_feat / (np.linalg.norm(item_feat, axis=1, keepdims=True) + 1e-8)\n\n# Dense similarity (sau đó sparse hóa top-K)\n# sim_item_item = item_feat @ item_feat.T\n# print(\"Shape sim_item_item:\", sim_item_item.shape)\n\n# K = 50\n# rows, cols, vals = [], [], []\n# for i in range(num_items):\n#     row = sim_item_item[i]\n#     topk_idx = np.argpartition(-row, min(K + 1, num_items - 1))[: K + 1]\n#     topk_idx = topk_idx[topk_idx != i]\n#     topk_idx = topk_idx[:K]\n#     for j in topk_idx:\n#         sim_ij = float(row[j])\n#         if sim_ij > 0:\n#             rows.append(i)\n#             cols.append(j)\n#             vals.append(sim_ij)\n\n# S_sparse = csr_matrix((vals, (rows, cols)), shape=(num_items, num_items), dtype=np.float32)\n# print(\"S_sparse: shape =\", S_sparse.shape,\n#       \"| nnz =\", S_sparse.nnz,\n#       \"| avg neighbors per item ~\", S_sparse.nnz / num_items)\n\n# Filter user + remap, build A_train_new và norm_adj_dev (tái dùng giữa dataset)\nMIN_TRAIN = 3\nMIN_TEST = 1\n\nkept_users = [\n    u for u in user_train_items.keys()\n    if len(user_train_items[u]) >= MIN_TRAIN and len(user_test_items.get(u, [])) >= MIN_TEST\n]\nprint(f\"[FILTER] kept users: {len(kept_users)} / {len(user_train_items)}\")\n\nold2new_user = {u_old: i for i, u_old in enumerate(sorted(kept_users))}\nnew2old_user = {i: u_old for u_old, i in old2new_user.items()}\nnum_users_new = len(old2new_user)\n\nuser_train_items_new, user_test_items_new = {}, {}\nfor u_old in kept_users:\n    u_new = old2new_user[u_old]\n    user_train_items_new[u_new] = list(user_train_items[u_old])\n    user_test_items_new[u_new] = list(user_test_items[u_old])\n\nrows, cols = [], []\nfor u_new, items in user_train_items_new.items():\n    rows.extend([u_new] * len(items))\n    cols.extend(items)\n\nA_train_new = csr_matrix(\n    (np.ones(len(rows), dtype=np.float32), (np.array(rows), np.array(cols))),\n    shape=(num_users_new, num_items),\n    dtype=np.float32\n)\nA_train_new.data[:] = 1.0\nA_train_new.eliminate_zeros()\nprint(f\"[MATRIX] A_train_new shape={A_train_new.shape} | nnz={A_train_new.nnz}\")\n\n# lambda_s = 0.3\n# A_train_soft = A_train_new @ S_sparse\n# A_train_soft = A_train_new + lambda_s * A_train_soft\n# A_train_soft.eliminate_zeros()\n\nA_for_adj = A_train_new      \n# A_for_adj = A_train_soft\n\nnorm_adj_sp_new = build_norm_adj_sparse(A_for_adj)\nnorm_adj_new = scipy_to_torch_sparse(norm_adj_sp_new)\n\nuser_train_items = user_train_items_new\nuser_test_items = user_test_items_new\nA_train = A_train_new\nnum_users = num_users_new\n\nnorm_adj_dev = norm_adj_new.to(device)\nprint(\"[READY] norm_adj_dev shape:\", norm_adj_dev.shape,\n      \"| num_users + num_items =\", num_users + num_items)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:41:40.557818Z","iopub.execute_input":"2025-12-11T10:41:40.558486Z","iopub.status.idle":"2025-12-11T10:41:40.651121Z","shell.execute_reply.started":"2025-12-11T10:41:40.558461Z","shell.execute_reply":"2025-12-11T10:41:40.650358Z"}},"outputs":[{"name":"stdout","text":"Số item có embedding CLIP: 10980 / 10986\n[FILTER] kept users: 1828 / 1956\n[MATRIX] A_train_new shape=(1828, 10986) | nnz=47102\n[READY] norm_adj_dev shape: torch.Size([12814, 12814]) | num_users + num_items = 12814\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"# NGCF","metadata":{}},{"cell_type":"code","source":"class NGCF(nn.Module):\n    def __init__(self, n_users, n_items, dim=128, layers=1, dropout=0.2):\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.dim = dim\n        self.layers = layers\n\n        self.embedding = nn.Embedding(n_users + n_items, dim)\n        nn.init.xavier_uniform_(self.embedding.weight)\n\n        self.W1 = nn.ModuleList([nn.Linear(dim, dim) for _ in range(layers)])\n        self.W2 = nn.ModuleList([nn.Linear(dim, dim) for _ in range(layers)])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, norm_adj):\n        x = self.embedding.weight\n        embs = [x]\n        for k in range(self.layers):\n            side = torch.sparse.mm(norm_adj, x)\n            sum_emb = self.W1[k](side + x)\n            bi_emb = self.W2[k](side * x)\n            x = F.leaky_relu(sum_emb + bi_emb, 0.2)\n            x = self.dropout(x)\n            x = F.normalize(x, dim=1)\n            embs.append(x)\n        return torch.cat(embs, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:40:40.367293Z","iopub.execute_input":"2025-12-11T10:40:40.367851Z","iopub.status.idle":"2025-12-11T10:40:40.374778Z","shell.execute_reply.started":"2025-12-11T10:40:40.367824Z","shell.execute_reply":"2025-12-11T10:40:40.373910Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"#H&M\n# ================== TRAIN NGCF (phiên bản nhẹ hơn) ==================\nngcf = NGCF(num_users, num_items, dim=64, layers=1, dropout=0.1).to(device)\noptimizer = torch.optim.AdamW(ngcf.parameters(), lr=0.005, weight_decay=1e-4)\nbpr = BPRLoss(reg_lambda=1e-4)\n\nepochs = 100              # giảm bớt cho đỡ nặng, sau chọn config tốt thì train lâu hơn\nrefresh_every = 10        # update hard negative mỗi 10 epoch\nn_samples_per_user = 20\nhard_ratio = 0.3\neval_every = 5\n\nloss_history_NGCF = []\nneighbors_idx = None\n\nprint(\"Training NGCF (light) ...\")\nfor epoch in range(epochs):\n    ngcf.train()\n\n    # 1) Forward 1 lần để lấy embedding (TRONG chế độ train)\n    emb_all = ngcf(norm_adj_dev)         # [U+I, D_all]\n    u_emb = emb_all[:num_users]\n    i_emb = emb_all[num_users:]\n\n    # 2) Build neighbors cho hard negative (ít thường xuyên hơn + nhẹ hơn)\n    with torch.no_grad():\n        if (epoch == 0) or (epoch % refresh_every == 0):\n            # dùng bản detach để không giữ graph\n            item_emb_cache = i_emb.detach()\n            neighbors_idx = build_item_neighbors(\n                item_emb_cache,\n                topk=50,     # giảm từ 100 -> 50\n                batch=512,   # giảm batch cho đỡ tốn RAM\n            )\n\n    # 3) Negative sampling\n    users, pos_items, neg_items = mixed_negative_sampling(\n        user_train_items,\n        num_items,\n        item_emb=i_emb.detach(),         # dùng embedding detach hiện tại\n        neighbors_idx=neighbors_idx,\n        n_samples_per_user=n_samples_per_user,\n        mix=hard_ratio,\n    )\n    users = users.to(device)\n    pos_items = pos_items.to(device)\n    neg_items = neg_items.to(device)\n\n    user_batch_emb = u_emb[users]\n    pos_emb = i_emb[pos_items]\n    neg_emb = i_emb[neg_items]\n\n    loss = bpr(user_batch_emb, pos_emb, neg_emb)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    loss_history_NGCF.append(loss.item())\n\n    # 4) Eval định kỳ (dùng eval mode, forward riêng nhưng không phải mỗi epoch)\n    if (epoch + 1) % eval_every == 0 or epoch == epochs - 1:\n        ngcf.eval()\n        with torch.no_grad():\n            emb_eval = ngcf(norm_adj_dev)\n            u_eval = emb_eval[:num_users]\n            i_eval = emb_eval[num_users:]\n            metrics = evaluate_embeddings(\n                u_eval, i_eval, user_train_items, user_test_items, k=10\n            )\n        print(\n            f\"Epoch {epoch+1:03d} | loss={loss.item():.4f} | \"\n            f\"HR@10={metrics['HitRate@10']:.4f} | \"\n            f\"Recall@10={metrics['Recall@10']:.4f} | \"\n            f\"NDCG@10={metrics['NDCG@10']:.4f} | \"\n            f\"Precision@10={metrics['Precision@10']:.4f} | \"\n            f\"Users_eval={metrics['Users_eval']}\"\n        )\n\nngcf.eval()\nwith torch.no_grad():\n    emb_final = ngcf(norm_adj_dev)\n    user_emb_ngcf = emb_final[:num_users]\n    item_emb_ngcf = emb_final[num_users:]\n\nprint(\"Done. user_emb_ngcf:\", user_emb_ngcf.shape,\n      \"| item_emb_ngcf:\", item_emb_ngcf.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:40:26.996038Z","iopub.execute_input":"2025-12-11T10:40:26.996370Z","iopub.status.idle":"2025-12-11T10:40:27.023007Z","shell.execute_reply.started":"2025-12-11T10:40:26.996345Z","shell.execute_reply":"2025-12-11T10:40:27.022047Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_362/814926662.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#H&M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ================== TRAIN NGCF (phiên bản nhẹ hơn) ==================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mngcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGCF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPRLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'NGCF' is not defined"],"ename":"NameError","evalue":"name 'NGCF' is not defined","output_type":"error"}],"execution_count":35},{"cell_type":"code","source":"ngcf = NGCF(num_users, num_items, dim=128, layers=2, dropout=0.1).to(device)\noptimizer = torch.optim.AdamW(ngcf.parameters(), lr=0.01, weight_decay=1e-4)\nbpr = BPRLoss(reg_lambda=1e-4)\n\nepochs = 200\nrefresh_every = 5\nn_samples_per_user = 20\nhard_ratio = 0.3\n\nloss_history_NGCF = []\nneighbors_idx = None\n\nprint(\"Training NGCF ...\")\nfor epoch in range(epochs):\n    ngcf.train()\n\n    with torch.no_grad():\n        emb_cache = ngcf(norm_adj_dev)\n        item_emb_cache = emb_cache[num_users:]\n        if (epoch == 0) or (epoch % refresh_every == 0):\n            neighbors_idx = build_item_neighbors(item_emb_cache, topk=100, batch=1024)\n\n    users, pos_items, neg_items = mixed_negative_sampling(\n        user_train_items,\n        num_items,\n        item_emb=item_emb_cache,\n        neighbors_idx=neighbors_idx,\n        n_samples_per_user=n_samples_per_user,\n        mix=hard_ratio,\n    )\n    users = users.to(device)\n    pos_items = pos_items.to(device)\n    neg_items = neg_items.to(device)\n\n    emb = ngcf(norm_adj_dev)\n    u_emb = emb[:num_users]\n    i_emb = emb[num_users:]\n\n    user_batch_emb = u_emb[users]\n    pos_emb = i_emb[pos_items]\n    neg_emb = i_emb[neg_items]\n\n    loss = bpr(user_batch_emb, pos_emb, neg_emb)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    loss_history_NGCF.append(loss.item())\n\n    if (epoch + 1) % 5  == 0:\n        ngcf.eval()\n        with torch.no_grad():\n            emb_eval = ngcf(norm_adj_dev)\n            u_eval = emb_eval[:num_users]\n            i_eval = emb_eval[num_users:]\n            metrics = evaluate_embeddings(\n                u_eval, i_eval, user_train_items, user_test_items, k=10\n            )\n        print(\n            f\"Epoch {epoch+1:03d} | loss={loss.item():.4f} | \"\n            f\"HR@10={metrics['HitRate@10']:.4f} | \"\n            f\"Recall@10={metrics['Recall@10']:.4f} | \"\n            f\"NDCG@10={metrics['NDCG@10']:.4f} | \"\n            f\"Precision@10={metrics['Precision@10']:.4f} | \"\n            f\"Users_eval={metrics['Users_eval']}\"\n        )\n\nngcf.eval()\nwith torch.no_grad():\n    emb_final = ngcf(norm_adj_dev)\n    user_emb_ngcf = emb_final[:num_users]\n    item_emb_ngcf = emb_final[num_users:]\n\nprint(\"Done. user_emb_ngcf:\", user_emb_ngcf.shape,\n      \"| item_emb_ngcf:\", item_emb_ngcf.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.makedirs(\"saved_models\", exist_ok=True)\n\nsave_path = \"saved_models/ngcf_model.pt\"\n\ntorch.save({\n    \"model_state_dict\": ngcf.state_dict(),\n    \"num_users\": num_users,\n    \"num_items\": num_items,\n    \"dim\": ngcf.dim,\n    \"layers\": ngcf.layers,\n    \"user_emb\": user_emb_ngcf.cpu(),\n    \"item_emb\": item_emb_ngcf.cpu(),\n}, save_path)\n\nprint(\"Saved NGCF model to:\", save_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load(\"saved_models/ngcf_model.pt\", map_location=device)\n\n# Khởi tạo model với đúng kiến trúc\nngcf_loaded = NGCF(\n    checkpoint[\"num_users\"],\n    checkpoint[\"num_items\"],\n    dim=checkpoint[\"dim\"],\n    layers=checkpoint[\"layers\"],\n    dropout=0.1\n).to(device)\n\nngcf_loaded.load_state_dict(checkpoint[\"model_state_dict\"])\nngcf_loaded.eval()\n\nprint(\"Loaded NGCF model.\")\nuser_emb_ngcf = checkpoint[\"user_emb\"].to(device)\nitem_emb_ngcf = checkpoint[\"item_emb\"].to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:40:45.353486Z","iopub.execute_input":"2025-12-11T10:40:45.354147Z","iopub.status.idle":"2025-12-11T10:40:45.401434Z","shell.execute_reply.started":"2025-12-11T10:40:45.354103Z","shell.execute_reply":"2025-12-11T10:40:45.400753Z"}},"outputs":[{"name":"stdout","text":"Loaded NGCF model.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"# LightGCN","metadata":{}},{"cell_type":"code","source":"# ================== LIGHTGCN MODEL ==================\nclass LightGCN(nn.Module):\n    def __init__(self, n_users, n_items, dim=128, n_layers=3):\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.dim = dim\n        self.n_layers = n_layers\n\n        # 1 embedding chung cho user + item\n        self.embedding = nn.Embedding(n_users + n_items, dim)\n        nn.init.xavier_uniform_(self.embedding.weight)\n\n    def forward(self, norm_adj):\n        \"\"\"\n        norm_adj: torch.sparse_coo_tensor (user+item graph)\n        return: embedding cuối cùng (user+item) sau khi average các layer\n        \"\"\"\n        all_embs = []\n        x = self.embedding.weight     # [n_users+n_items, dim]\n        all_embs.append(x)\n\n        for _ in range(self.n_layers):\n            x = torch.sparse.mm(norm_adj, x)   # propagate\n            x = F.normalize(x, dim=1)\n            all_embs.append(x)\n\n        # stack [n_nodes, n_layers+1, dim] rồi mean theo layer\n        embs = torch.stack(all_embs, dim=1)\n        out = torch.mean(embs, dim=1)\n        out = F.normalize(out, dim=1)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:40:51.757186Z","iopub.execute_input":"2025-12-11T10:40:51.757810Z","iopub.status.idle":"2025-12-11T10:40:51.763788Z","shell.execute_reply.started":"2025-12-11T10:40:51.757786Z","shell.execute_reply":"2025-12-11T10:40:51.762926Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# ================== TRAIN LIGHTGCN ==================\nlightgcn = LightGCN(num_users, num_items, dim=128, n_layers=3).to(device)\noptimizer_lg = torch.optim.AdamW(lightgcn.parameters(), lr=0.01, weight_decay=1e-4)\nbpr_lg = BPRLoss(reg_lambda=1e-4)\n\nepochs_lg = 200\nrefresh_every_lg = 5\nn_samples_per_user_lg = 20\nhard_ratio_lg = 0.3\n\nloss_history_LightGCN = []\nneighbors_idx_lg = None\n\nprint(\"Training LightGCN ...\")\nfor epoch in range(epochs_lg):\n    lightgcn.train()\n\n    # cache embedding + neighbors cho hard negative\n    with torch.no_grad():\n        emb_cache = lightgcn(norm_adj_dev)\n        item_emb_cache = emb_cache[num_users:]\n        if (epoch == 0) or (epoch % refresh_every_lg == 0):\n            neighbors_idx_lg = build_item_neighbors(item_emb_cache, topk=100, batch=1024)\n\n    # negative sampling (mixed hard + random giống NGCF)\n    users, pos_items, neg_items = mixed_negative_sampling(\n        user_train_items,\n        num_items,\n        item_emb=item_emb_cache,\n        neighbors_idx=neighbors_idx_lg,\n        n_samples_per_user=n_samples_per_user_lg,\n        mix=hard_ratio_lg,\n    )\n    users = users.to(device)\n    pos_items = pos_items.to(device)\n    neg_items = neg_items.to(device)\n\n    emb = lightgcn(norm_adj_dev)\n    u_emb = emb[:num_users]\n    i_emb = emb[num_users:]\n\n    user_batch_emb = u_emb[users]\n    pos_emb = i_emb[pos_items]\n    neg_emb = i_emb[neg_items]\n\n    loss = bpr_lg(user_batch_emb, pos_emb, neg_emb)\n    optimizer_lg.zero_grad()\n    loss.backward()\n    optimizer_lg.step()\n\n    loss_history_LightGCN.append(loss.item())\n\n    if (epoch + 1) % 5 == 0:\n        lightgcn.eval()\n        with torch.no_grad():\n            emb_eval = lightgcn(norm_adj_dev)\n            u_eval = emb_eval[:num_users]\n            i_eval = emb_eval[num_users:]\n            metrics_lightgcn = evaluate_embeddings(\n                u_eval, i_eval, user_train_items, user_test_items, k=10\n            )\n        print(\n            f\"[LightGCN] Epoch {epoch+1:03d} | loss={loss.item():.4f} | \"\n            f\"HR@10={metrics_lightgcn['HitRate@10']:.4f} | \"\n            f\"Recall@10={metrics_lightgcn['Recall@10']:.4f} | \"\n            f\"NDCG@10={metrics_lightgcn['NDCG@10']:.4f} | \"\n            f\"Precision@10={metrics_lightgcn['Precision@10']:.4f} | \"\n            f\"Users_eval={metrics_lightgcn['Users_eval']}\"\n        )\n\nlightgcn.eval()\nwith torch.no_grad():\n    emb_lg_final = lightgcn(norm_adj_dev)\n    user_emb_lg = emb_lg_final[:num_users]\n    item_emb_lg = emb_lg_final[num_users:]\n\nprint(\"Done LightGCN. user_emb_lg:\", user_emb_lg.shape,\n      \"| item_emb_lg:\", item_emb_lg.shape)\n\n# Đánh giá LightGCN-only (lưu lại để tổng hợp báo cáo)\nmetrics_lightgcn = evaluate_embeddings(\n    user_emb_lg, item_emb_lg,\n    user_train_items, user_test_items,\n    k=10\n)\nprint(\"LightGCN only:\", metrics_lightgcn)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_cf_model(\n    \"lightgcn\",\n    model=lightgcn,\n    user_emb=user_emb_lg,\n    item_emb=item_emb_lg,\n    extra_info={\n        \"num_users\": num_users,\n        \"num_items\": num_items,\n        \"dim\": lightgcn.dim,\n        \"n_layers\": lightgcn.n_layers,\n    }\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ALS","metadata":{}},{"cell_type":"code","source":"# ================== ALS (Implicit 0/1) ==================\ndef train_als_implicit(A_csr, n_factors=64, n_iters=15, reg=0.1, seed=42):\n    \"\"\"\n    A_csr: scipy.sparse.csr_matrix [num_users, num_items] với giá trị 0/1\n    n_factors: số chiều latent\n    n_iters: số vòng lặp ALS\n    reg: hệ số regularization\n    Trả về:\n        P: user_factors [num_users, n_factors]\n        Q: item_factors [num_items, n_factors]\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    n_users, n_items = A_csr.shape\n\n    P = 0.01 * rng.randn(n_users, n_factors).astype(np.float32)\n    Q = 0.01 * rng.randn(n_items, n_factors).astype(np.float32)\n\n    # dùng transpose để update item\n    A_csc = A_csr.T.tocsr()\n\n    I_f = np.eye(n_factors, dtype=np.float32)\n\n    for it in range(n_iters):\n        t0 = time.time()\n\n        # ---- update user factors ----\n        QTQ = Q.T @ Q      # [f, f]\n        for u in range(n_users):\n            start, end = A_csr.indptr[u], A_csr.indptr[u + 1]\n            idx_items = A_csr.indices[start:end]\n            if len(idx_items) == 0:\n                continue\n            Y = Q[idx_items]                 # [n_i, f]\n            A = QTQ + reg * I_f + (Y.T @ Y - QTQ)   # ≈ Y.T @ Y + reg * I\n            b = Y.T @ np.ones(len(idx_items), dtype=np.float32)\n            P[u] = np.linalg.solve(A, b)\n\n        # ---- update item factors ----\n        PTP = P.T @ P\n        for i in range(n_items):\n            start, end = A_csc.indptr[i], A_csc.indptr[i + 1]\n            idx_users = A_csc.indices[start:end]\n            if len(idx_users) == 0:\n                continue\n            X = P[idx_users]                 # [n_u, f]\n            A = PTP + reg * I_f + (X.T @ X - PTP)\n            b = X.T @ np.ones(len(idx_users), dtype=np.float32)\n            Q[i] = np.linalg.solve(A, b)\n\n        print(f\"[ALS] iter {it+1}/{n_iters} done in {time.time()-t0:.1f}s\")\n\n    return P, Q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train ALS trên A_train\nals_factors = 64\nals_iters = 15\nals_reg = 0.1\n\nprint(\"Training ALS (implicit 0/1) ...\")\nP_als, Q_als = train_als_implicit(\n    A_train,\n    n_factors=als_factors,\n    n_iters=als_iters,\n    reg=als_reg,\n)\n\n# Chuyển sang torch để dùng lại evaluate_embeddings\nuser_emb_als = torch.from_numpy(P_als).float().to(device)\nitem_emb_als = torch.from_numpy(Q_als).float().to(device)\n\nprint(\"ALS user_emb:\", user_emb_als.shape,\n      \"| ALS item_emb:\", item_emb_als.shape)\n\nmetrics_als = evaluate_embeddings(\n    user_emb_als, item_emb_als,\n    user_train_items, user_test_items,\n    k=10\n)\nprint(\"ALS only:\", metrics_als)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# user_emb_als, item_emb_als = torch.from_numpy(P), torch.from_numpy(Q)\nsave_cf_model(\n    \"als\",\n    model=None,                     # ALS không có state_dict\n    user_emb=user_emb_als,\n    item_emb=item_emb_als,\n    extra_info={\n        \"num_users\": user_emb_als.shape[0],\n        \"num_items\": item_emb_als.shape[0],\n        \"dim\": user_emb_als.shape[1],\n    }\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load model","metadata":{}},{"cell_type":"code","source":"lightgcn_loaded, user_emb_lg, item_emb_lg, info_lg = load_cf_model(\n    \"lightgcn\",\n    ModelClass=LightGCN,\n    device=device\n)\n_, user_emb_als, item_emb_als, info_als = load_cf_model(\n    \"als\",\n    ModelClass=None,\n    device=device\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:41:17.732811Z","iopub.execute_input":"2025-12-11T10:41:17.733070Z","iopub.status.idle":"2025-12-11T10:41:17.833614Z","shell.execute_reply.started":"2025-12-11T10:41:17.733054Z","shell.execute_reply":"2025-12-11T10:41:17.832831Z"}},"outputs":[{"name":"stdout","text":"[OK] Loaded lightgcn from saved_models/lightgcn.pt\n[OK] Loaded als from saved_models/als.pt\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"# CBFRS","metadata":{}},{"cell_type":"code","source":"# ==== CBF: Chuẩn bị content embedding cho item (từ CLIP) ====\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\n# item_feat: np.ndarray [num_items, feat_dim] đã build ở trên\n# has_emb:   np.ndarray [num_items] bool\n\nitem_feat_t = torch.from_numpy(item_feat).float().to(device)   # [num_items, D]\nitem_feat_t = F.normalize(item_feat_t, p=2, dim=1)             # đảm bảo L2-norm\n\nhas_emb_t = torch.from_numpy(has_emb).to(device)               # [num_items]\nprint(\"CBF item_feat_t:\", item_feat_t.shape, \"| has_emb_t:\", has_emb_t.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:41:50.404866Z","iopub.execute_input":"2025-12-11T10:41:50.405407Z","iopub.status.idle":"2025-12-11T10:41:50.458624Z","shell.execute_reply.started":"2025-12-11T10:41:50.405380Z","shell.execute_reply":"2025-12-11T10:41:50.457927Z"}},"outputs":[{"name":"stdout","text":"CBF item_feat_t: torch.Size([10986, 512]) | has_emb_t: torch.Size([10986])\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# ==== CBF: build user profile + recommend + evaluate (dùng chung train/test với CF) ====\n\ndef build_user_profile_cbf(u, user_train_items, item_feat_t, has_emb_t=None):\n    \"\"\"\n    Xây profile nội dung cho user u bằng trung bình embedding CLIP\n    của các item user đó đã tương tác (trong TRAIN).\n\n    u: user_id đã encode (0..num_users-1)\n    user_train_items: dict[u] -> list[item_id]\n    item_feat_t: torch.Tensor [num_items, D] (đã normalize)\n    has_emb_t: optional, torch.BoolTensor [num_items] (item nào có embedding)\n    \"\"\"\n    items = user_train_items.get(u, [])\n    if not items:\n        return None\n\n    items = torch.tensor(items, dtype=torch.long, device=item_feat_t.device)\n\n    if has_emb_t is not None:\n        mask = has_emb_t[items]\n        items = items[mask]\n        if items.numel() == 0:\n            return None\n\n    embs = item_feat_t[items]        # [n, D]\n    prof = embs.mean(dim=0)          # [D]\n    prof = F.normalize(prof, p=2, dim=0)\n    return prof\n\n\ndef recommend_cbf_for_user_cf_idx(\n    u,\n    user_train_items,\n    item_feat_t,\n    has_emb_t=None,\n    topk=10,\n    exclude_seen=True,\n):\n    \"\"\"\n    Recommend THUẦN CBF cho user u (ID đã encode).\n    Trả về list (item_id, score).\n    \"\"\"\n    user_prof = build_user_profile_cbf(u, user_train_items, item_feat_t, has_emb_t)\n    if user_prof is None:\n        return []\n\n    scores = item_feat_t @ user_prof   # [num_items], cosine vì đã normalize\n\n    # loại các item đã thấy trong train nếu cần\n    if exclude_seen:\n        seen = set(user_train_items.get(u, []))\n        if seen:\n            seen_idx = torch.tensor(list(seen), dtype=torch.long, device=item_feat_t.device)\n            scores[seen_idx] = -1e9\n\n    # nếu có has_emb_t, loại item không có embedding\n    if has_emb_t is not None:\n        scores[~has_emb_t] = -1e9\n\n    # top-k\n    topk = min(topk, scores.shape[0])\n    top_scores, top_idx = torch.topk(scores, k=topk)\n\n    recs = [(int(top_idx[i].item()), float(top_scores[i].item())) for i in range(topk)]\n    return recs\n\n\ndef evaluate_cbf_cf_idx(\n    item_feat_t,\n    user_train_items,\n    user_test_items,\n    has_emb_t=None,\n    k=10,\n):\n    \"\"\"\n    Đánh giá CBF-only với cùng split train/test như CF.\n    Trả về Precision@k, Recall@k, NDCG@k, HitRate@k.\n    \"\"\"\n    item_feat_n = F.normalize(item_feat_t, p=2, dim=1)\n\n    precisions, recalls, ndcgs, hit_rates = [], [], [], []\n\n    for u in user_test_items.keys():\n        test_items = set(user_test_items.get(u, []))\n        if not test_items:\n            continue\n\n        user_prof = build_user_profile_cbf(u, user_train_items, item_feat_n, has_emb_t)\n        if user_prof is None:\n            continue\n\n        scores = item_feat_n @ user_prof   # [num_items]\n\n        train_items = set(user_train_items.get(u, []))\n        if train_items:\n            train_idx = torch.tensor(list(train_items), dtype=torch.long, device=item_feat_t.device)\n            scores[train_idx] = -1e9\n\n        if has_emb_t is not None:\n            scores[~has_emb_t] = -1e9\n\n        top_k = torch.topk(scores, k=min(k, scores.shape[0])).indices.cpu().numpy()\n        hits = len(set(top_k) & test_items)\n\n        precision = hits / k\n        recall = hits / len(test_items)\n\n        # NDCG\n        dcg = 0.0\n        for rank, iid in enumerate(top_k):\n            if iid in test_items:\n                dcg += 1.0 / np.log2(rank + 2)\n        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(test_items), k)))\n        ndcg = dcg / idcg if idcg > 0 else 0.0\n\n        hit = 1.0 if hits > 0 else 0.0\n\n        precisions.append(precision)\n        recalls.append(recall)\n        ndcgs.append(ndcg)\n        hit_rates.append(hit)\n\n    return {\n        f\"Precision@{k}\": float(np.mean(precisions)) if precisions else 0.0,\n        f\"Recall@{k}\": float(np.mean(recalls)) if recalls else 0.0,\n        f\"NDCG@{k}\": float(np.mean(ndcgs)) if ndcgs else 0.0,\n        f\"HitRate@{k}\": float(np.mean(hit_rates)) if hit_rates else 0.0,\n        \"Users_eval\": int(len(precisions)),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:41:51.365060Z","iopub.execute_input":"2025-12-11T10:41:51.365630Z","iopub.status.idle":"2025-12-11T10:41:51.379556Z","shell.execute_reply.started":"2025-12-11T10:41:51.365604Z","shell.execute_reply":"2025-12-11T10:41:51.378768Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"metrics_cbf = evaluate_cbf_cf_idx(\n    item_feat_t,\n    user_train_items,\n    user_test_items,\n    has_emb_t=has_emb_t,\n    k=10\n)\nprint(\"CBF only:\", metrics_cbf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:41:54.715625Z","iopub.execute_input":"2025-12-11T10:41:54.716135Z","iopub.status.idle":"2025-12-11T10:41:56.068395Z","shell.execute_reply.started":"2025-12-11T10:41:54.716107Z","shell.execute_reply":"2025-12-11T10:41:56.067647Z"}},"outputs":[{"name":"stdout","text":"CBF only: {'Precision@10': 0.0019146608315098468, 'Recall@10': 0.004347886492306624, 'NDCG@10': 0.003707473113997233, 'HitRate@10': 0.019146608315098467, 'Users_eval': 1828}\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# ==== Ensemble CF (LightGCN) + CBF trên cùng split train/test ====\n\ndef evaluate_cf_cbf_ensemble(\n    user_emb_lg,          # user_emb_lg: [num_users, D_cf]\n    item_emb_lg,          # item_emb_lg: [num_items, D_cf]\n    item_feat_t,          # content embedding (text-only CBF): [num_items, D_cb]\n    user_train_items,\n    user_test_items,\n    has_emb_t=None,\n    alpha_cf=0.7,         # trọng số CF (LightGCN)\n    alpha_cb=0.3,         # trọng số CBF\n    k=10,\n):\n    # Chuẩn hóa embedding CF (LightGCN) và CBF\n    U_cf = F.normalize(user_emb_lg, dim=1)        # [num_users, D_cf]\n    I_cf = F.normalize(item_emb_lg, dim=1)        # [num_items, D_cf]\n    I_cb = F.normalize(item_feat_t, dim=1)        # [num_items, D_cb]\n\n    precisions, recalls, ndcgs, hit_rates = [], [], [], []\n\n    for u in user_test_items.keys():\n        test_items = set(user_test_items.get(u, []))\n        if not test_items:\n            continue\n\n        # --- CF scores (LightGCN) ---\n        scores_cf = (I_cf @ U_cf[u]).clone()      # [num_items]\n\n        # --- CBF scores ---\n        user_prof_cb = build_user_profile_cbf(u, user_train_items, I_cb, has_emb_t)\n        if user_prof_cb is None:\n            scores_cb = torch.zeros_like(scores_cf)\n        else:\n            scores_cb = I_cb @ user_prof_cb       # [num_items]\n\n        # --- Ensemble ---\n        scores = alpha_cf * scores_cf + alpha_cb * scores_cb\n\n        # loại item đã thấy trong train\n        train_items = set(user_train_items.get(u, []))\n        if train_items:\n            train_idx = torch.tensor(\n                list(train_items), dtype=torch.long, device=item_feat_t.device\n            )\n            scores[train_idx] = -1e9\n\n        # nếu chỉ một phần item có content embedding\n        if has_emb_t is not None:\n            scores[~has_emb_t] = -1e9\n\n        # top-k\n        top_k = torch.topk(scores, k=min(k, scores.shape[0])).indices.cpu().numpy()\n        hits = len(set(top_k) & test_items)\n\n        precision = hits / k\n        recall = hits / len(test_items)\n\n        # NDCG\n        dcg = 0.0\n        for rank, iid in enumerate(top_k):\n            if iid in test_items:\n                dcg += 1.0 / np.log2(rank + 2)\n        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(test_items), k)))\n        ndcg = dcg / idcg if idcg > 0 else 0.0\n\n        hit = 1.0 if hits > 0 else 0.0\n\n        precisions.append(precision)\n        recalls.append(recall)\n        ndcgs.append(ndcg)\n        hit_rates.append(hit)\n\n    return {\n        f\"Precision@{k}\": float(np.mean(precisions)) if precisions else 0.0,\n        f\"Recall@{k}\": float(np.mean(recalls)) if recalls else 0.0,\n        f\"NDCG@{k}\": float(np.mean(ndcgs)) if ndcgs else 0.0,\n        f\"HitRate@{k}\": float(np.mean(hit_rates)) if hit_rates else 0.0,\n        \"Users_eval\": int(len(precisions)),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:42:05.370624Z","iopub.execute_input":"2025-12-11T10:42:05.370922Z","iopub.status.idle":"2025-12-11T10:42:05.383162Z","shell.execute_reply.started":"2025-12-11T10:42:05.370900Z","shell.execute_reply":"2025-12-11T10:42:05.382395Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# ================== CF-only ==================\n# NGCF\nmetrics_ngcf = evaluate_embeddings(\n    user_emb_ngcf, item_emb_ngcf,\n    user_train_items, user_test_items,\n    k=10\n)\nprint(\"NGCF only:\", metrics_ngcf)\n\n# LightGCN\nmetrics_lg = evaluate_embeddings(\n    user_emb_lg, item_emb_lg,\n    user_train_items, user_test_items,\n    k=10\n)\nprint(\"LightGCN only:\", metrics_lg)\n\n# ALS\nmetrics_als = evaluate_embeddings(\n    user_emb_als, item_emb_als,\n    user_train_items, user_test_items,\n    k=10\n)\nprint(\"ALS only:\", metrics_als)\n\n\n# ================== CBF-only ==================\nmetrics_cbf = evaluate_cbf_cf_idx(\n    item_feat_t,\n    user_train_items, user_test_items,\n    has_emb_t=has_emb_t,\n    k=10\n)\nprint(\"CBF only:\", metrics_cbf)\n\n\n# ================== Ensemble CF + CBF ==================\n# NGCF + CBF\nmetrics_ens_ngcf = evaluate_cf_cbf_ensemble(\n    user_emb_ngcf, item_emb_ngcf,\n    item_feat_t,\n    user_train_items, user_test_items,\n    has_emb_t=has_emb_t,\n    alpha_cf=0.7,\n    alpha_cb=0.3,\n    k=10\n)\nprint(\"Ensemble NGCF + CBF:\", metrics_ens_ngcf)\n\n# LightGCN + CBF\nmetrics_ens_lg = evaluate_cf_cbf_ensemble(\n    user_emb_lg, item_emb_lg,\n    item_feat_t,\n    user_train_items, user_test_items,\n    has_emb_t=has_emb_t,\n    alpha_cf=0.7,\n    alpha_cb=0.3,\n    k=10\n)\nprint(\"Ensemble LightGCN + CBF:\", metrics_ens_lg)\n\n# ALS + CBF\nmetrics_ens_als = evaluate_cf_cbf_ensemble(\n    user_emb_als, item_emb_als,\n    item_feat_t,\n    user_train_items, user_test_items,\n    has_emb_t=has_emb_t,\n    alpha_cf=0.7,\n    alpha_cb=0.3,\n    k=10\n)\nprint(\"Ensemble ALS + CBF:\", metrics_ens_als)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T10:42:06.379954Z","iopub.execute_input":"2025-12-11T10:42:06.380609Z","iopub.status.idle":"2025-12-11T10:42:41.701771Z","shell.execute_reply.started":"2025-12-11T10:42:06.380580Z","shell.execute_reply":"2025-12-11T10:42:41.700987Z"}},"outputs":[{"name":"stdout","text":"NGCF only: {'Precision@10': 0.018161925601750548, 'Recall@10': 0.036449981619190996, 'NDCG@10': 0.028554664316951677, 'HitRate@10': 0.14387308533916848, 'Users_eval': 1828}\nLightGCN only: {'Precision@10': 0.01849015317286652, 'Recall@10': 0.03866659575480487, 'NDCG@10': 0.03233667147485906, 'HitRate@10': 0.15590809628008753, 'Users_eval': 1828}\nALS only: {'Precision@10': 0.013785557986870896, 'Recall@10': 0.031093409672675053, 'NDCG@10': 0.024891640492193882, 'HitRate@10': 0.11214442013129103, 'Users_eval': 1828}\nCBF only: {'Precision@10': 0.0019146608315098468, 'Recall@10': 0.004347886492306624, 'NDCG@10': 0.003707473113997233, 'HitRate@10': 0.019146608315098467, 'Users_eval': 1828}\nEnsemble NGCF + CBF: {'Precision@10': 0.0028993435448577683, 'Recall@10': 0.005826424700727855, 'NDCG@10': 0.004444511168844332, 'HitRate@10': 0.027899343544857767, 'Users_eval': 1828}\nEnsemble LightGCN + CBF: {'Precision@10': 0.005306345733041575, 'Recall@10': 0.012202881182853799, 'NDCG@10': 0.008645646415717403, 'HitRate@10': 0.04923413566739606, 'Users_eval': 1828}\nEnsemble ALS + CBF: {'Precision@10': 0.0027352297592997815, 'Recall@10': 0.007649461587705519, 'NDCG@10': 0.005350851628971291, 'HitRate@10': 0.026805251641137857, 'Users_eval': 1828}\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# Popularity: số lần xuất hiện trong A_train\nitem_pop = np.asarray(A_train.sum(axis=0)).flatten().astype(np.float32)\nif item_pop.max() > 0:\n    item_pop_norm = item_pop / (item_pop.max() + 1e-8)\nelse:\n    item_pop_norm = np.zeros_like(item_pop, dtype=np.float32)\n\nitem_pop_t = torch.from_numpy(item_pop_norm).float().to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def recommend_popular(topk=10, exclude_items=None):\n    scores = item_pop_t.clone()\n    if exclude_items:\n        idx = torch.tensor(list(exclude_items), dtype=torch.long, device=item_pop_t.device)\n        scores[idx] = -1e9\n    k = min(topk, scores.shape[0])\n    top_scores, top_idx = torch.topk(scores, k=k)\n    return [(int(top_idx[i].item()), float(top_scores[i].item())) for i in range(k)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def recommend_for_user(\n    u,\n    user_train_items,\n    user_emb_cf,\n    item_emb_cf,\n    item_feat_t,\n    has_emb_t=None,\n    topk=10,\n    min_interactions_cf=3,\n):\n    \"\"\"\n    u: user_id đã encode (0..num_users-1) và đã đi qua remap kept_users\n    \"\"\"\n\n    # 1) Lấy history của user\n    history = user_train_items.get(u, [])\n    n_hist = len(history)\n\n    # Nếu user không có trong train (trong thực tế online gặp, trong offline ít gặp)\n    if n_hist == 0:\n        # cold user hoàn toàn: recommend theo POP\n        return recommend_popular(topk=topk, exclude_items=None)\n\n    # 2) Xác định trọng số CF / CBF tùy theo độ dày history\n    if n_hist < min_interactions_cf:\n        # lịch sử mỏng: ưu tiên CBF\n        alpha_cf = 0.3\n        alpha_cb = 0.7\n    else:\n        # lịch sử đủ dày: CF mạnh hơn\n        alpha_cf = 0.7\n        alpha_cb = 0.3\n\n    # 3) Chuẩn hóa embedding CF / CBF\n    U_cf = F.normalize(user_emb_cf, dim=1)\n    I_cf = F.normalize(item_emb_cf, dim=1)\n    I_cb = F.normalize(item_feat_t, p=2, dim=1)\n\n    # --- CF scores ---\n    scores_cf = (I_cf @ U_cf[u]).clone()  # [num_items]\n\n    # --- CBF scores ---\n    user_prof_cb = build_user_profile_cbf(u, user_train_items, I_cb, has_emb_t)\n    if user_prof_cb is None:\n        scores_cb = torch.zeros_like(scores_cf)\n    else:\n        scores_cb = I_cb @ user_prof_cb\n\n    # --- Ensemble CF + CBF ---\n    scores = alpha_cf * scores_cf + alpha_cb * scores_cb\n\n    # Mask các item đã xem\n    seen = set(history)\n    if seen:\n        seen_idx = torch.tensor(list(seen), dtype=torch.long, device=item_feat_t.device)\n        scores[seen_idx] = -1e9\n\n    # Không dùng item không có content nếu muốn\n    if has_emb_t is not None:\n        scores[~has_emb_t] = -1e9\n\n    # 4) Nếu user quá lạnh, có thể blend thêm POP\n    # (option): scores += gamma * item_pop_t\n    # Ví dụ cho user rất mới:\n    if n_hist < 2:\n        gamma = 0.2\n        scores = scores + gamma * item_pop_t\n\n    # top-k\n    k = min(topk, scores.shape[0])\n    top_scores, top_idx = torch.topk(scores, k=k)\n    recs = [(int(top_idx[i].item()), float(top_scores[i].item())) for i in range(k)]\n    return recs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\ni = random.randint(1, 1828)   \nu_test = list(user_test_items.keys())[i]\n\nrecs = recommend_for_user(\n    u_test,\n    user_train_items,\n    user_emb_ngcf,\n    item_emb_ngcf,\n    item_feat_t,\n    has_emb_t=has_emb_t,\n    topk=10,\n    min_interactions_cf=3,\n)\nprint(\"Hybrid recs for user\", u_test, \":\", recs[:5])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = {\n    \"NGCF\": metrics_ngcf,\n    \"LightGCN\": metrics_lg,\n    \"ALS\": metrics_als,\n    \"CBF\": metrics_cbf,\n    \"NGCF+CBF\": metrics_ens_ngcf,\n    \"LightGCN+CBF\": metrics_ens_lg,\n    \"ALS+CBF\": metrics_ens_als,\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nmetrics = [\"Precision@10\", \"Recall@10\", \"NDCG@10\", \"HitRate@10\"]\nmodel_names = list(results.keys())\n\n# tạo ma trận giá trị: rows = metrics, cols = models\nvalues = np.array([[results[m][metric] for m in model_names] for metric in metrics])\n\nplt.figure(figsize=(16, 8))\n\nx = np.arange(len(model_names))\nwidth = 0.18\n\nfor i, metric in enumerate(metrics):\n    plt.bar(x + i*width, values[i], width, label=metric)\n\nplt.xticks(x + width*1.5, model_names, rotation=45)\nplt.ylabel(\"Score\")\nplt.title(\"So sánh các mô hình CF, CBF và Ensemble (k=10)\")\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16, 12))\n\nfor i, metric in enumerate(metrics):\n    plt.subplot(2, 2, i+1)\n    vals = [results[m][metric] for m in model_names]\n    plt.bar(model_names, vals, color='skyblue')\n    plt.title(metric)\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}